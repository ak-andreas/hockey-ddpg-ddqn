{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import hockey.hockey_env as hockey\n",
    "from memory import Memory\n",
    "from feedforward import Feedforward\n",
    "from per_memory import PERMemory\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "class UnsupportedSpace(Exception):\n",
    "    \"\"\"Exception for an unsupported observation or action space.\"\"\"\n",
    "    def __init__(self, message=\"Unsupported Space\"):\n",
    "        super().__init__(message)\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    \"\"\"Q-function that uses a feedforward neural network.\"\"\"\n",
    "    def __init__(self, observation_dim, action_dim,\n",
    "                 hidden_sizes=[100, 100],\n",
    "                 learning_rate=0.0002):\n",
    "        super().__init__()\n",
    "        self.net = Feedforward(\n",
    "            input_size=observation_dim + action_dim,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=1\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=learning_rate,\n",
    "            eps=1e-6\n",
    "        )\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction='none')\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def fit(self, observations, actions, targets, weights=None):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.Q_value(observations, actions)\n",
    "        loss_unreduced = self.loss_fn(pred, targets)\n",
    "        if weights is not None:\n",
    "            loss = (loss_unreduced * weights).mean()\n",
    "        else:\n",
    "            loss = loss_unreduced.mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        td_error = pred - targets\n",
    "        return loss.item(), td_error.detach().cpu().numpy()\n",
    "\n",
    "    def Q_value(self, observations, actions):\n",
    "        return self.forward(torch.cat([observations, actions], dim=-1))\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise for exploration.\"\"\"\n",
    "    def __init__(self, shape, theta=0.15, dt=1e-2):\n",
    "        self._shape = shape\n",
    "        self._theta = theta\n",
    "        self._dt = dt\n",
    "        self.noise_prev = np.zeros(self._shape)\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        noise = (\n",
    "            self.noise_prev\n",
    "            + self._theta * (-self.noise_prev) * self._dt\n",
    "            + np.sqrt(self._dt) * np.random.normal(size=self._shape)\n",
    "        )\n",
    "        self.noise_prev = noise\n",
    "        return noise\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise_prev = np.zeros(self._shape)\n",
    "\n",
    "class DDPGAgent:\n",
    "    \"\"\"DDPG agent with neural networks for Q and policy. Uses PER if 'use_per' is True.\"\"\"\n",
    "    def __init__(self, observation_space, action_space, **userconfig):\n",
    "        if not isinstance(observation_space, spaces.Box):\n",
    "            raise UnsupportedSpace(f'Observation space {observation_space} incompatible.')\n",
    "        if not isinstance(action_space, spaces.Box):\n",
    "            raise UnsupportedSpace(f'Action space {action_space} incompatible.')\n",
    "\n",
    "        self.device = device\n",
    "        self._obs_dim = observation_space.shape[0]\n",
    "        self._action_dim = 4\n",
    "        self._action_space = Box(\n",
    "            low=action_space.low[:4],\n",
    "            high=action_space.high[:4],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self._config = {\n",
    "            \"eps\": 0.05,\n",
    "            \"discount\": 0.95,\n",
    "            \"buffer_size\": int(1e6),\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate_actor\": 0.0003,\n",
    "            \"learning_rate_critic\": 0.0003,\n",
    "            \"hidden_sizes_actor\": [256, 256],\n",
    "            \"hidden_sizes_critic\": [256, 256],\n",
    "            \"update_target_every\": 100,\n",
    "            \"use_target_net\": True,\n",
    "            \"total_episodes\": 50000,\n",
    "            \"seed\": 0,\n",
    "            \"tau\": 0.005,\n",
    "            \"use_per\": True\n",
    "        }\n",
    "        self._config.update(userconfig)\n",
    "\n",
    "        self.eps = self._config[\"eps\"]\n",
    "        self.discount = self._config[\"discount\"]\n",
    "        self.batch_size = self._config[\"batch_size\"]\n",
    "        self.buffer_size = self._config[\"buffer_size\"]\n",
    "        self.tau = self._config[\"tau\"]\n",
    "        self.use_target_net = self._config[\"use_target_net\"]\n",
    "        self.update_target_every = self._config[\"update_target_every\"]\n",
    "        self.train_iter = 0\n",
    "        self.use_per = self._config[\"use_per\"]\n",
    "\n",
    "        if self.use_per:\n",
    "            self.buffer = PERMemory(\n",
    "                obs_dim=self._obs_dim,\n",
    "                act_dim=self._action_dim,\n",
    "                max_size=self.buffer_size,\n",
    "                device=self.device\n",
    "            )\n",
    "        else:\n",
    "            self.buffer = Memory(\n",
    "                obs_dim=self._obs_dim,\n",
    "                act_dim=self._action_dim,\n",
    "                max_size=self.buffer_size,\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "        self.Q = QFunction(\n",
    "            observation_dim=self._obs_dim,\n",
    "            action_dim=self._action_dim,\n",
    "            hidden_sizes=self._config[\"hidden_sizes_critic\"],\n",
    "            learning_rate=self._config[\"learning_rate_critic\"]\n",
    "        )\n",
    "        self.Q_target = QFunction(\n",
    "            observation_dim=self._obs_dim,\n",
    "            action_dim=self._action_dim,\n",
    "            hidden_sizes=self._config[\"hidden_sizes_critic\"],\n",
    "            learning_rate=0\n",
    "        )\n",
    "\n",
    "        self.policy = Feedforward(\n",
    "            input_size=self._obs_dim,\n",
    "            hidden_sizes=self._config[\"hidden_sizes_actor\"],\n",
    "            output_size=self._action_dim,\n",
    "            activation_fun=nn.ReLU(),\n",
    "            output_activation=nn.Tanh()\n",
    "        )\n",
    "        self.policy_target = Feedforward(\n",
    "            input_size=self._obs_dim,\n",
    "            hidden_sizes=self._config[\"hidden_sizes_actor\"],\n",
    "            output_size=self._action_dim,\n",
    "            activation_fun=nn.ReLU(),\n",
    "            output_activation=nn.Tanh()\n",
    "        )\n",
    "        self.policy.to(self.device)\n",
    "        self.policy_target.to(self.device)\n",
    "\n",
    "        self._copy_nets()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.policy.parameters(),\n",
    "            lr=0.0001,\n",
    "            eps=1e-3\n",
    "        )\n",
    "        self.action_noise = OUNoise((self._action_dim,), theta=0.3, dt=0.02)\n",
    "\n",
    "    def _copy_nets(self):\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "        self.policy_target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, param in zip(self.Q_target.parameters(), self.Q.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "            )\n",
    "        for target_param, param in zip(self.policy_target.parameters(), self.policy.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "            )\n",
    "\n",
    "    def act(self, observation, eps=None):\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        obs_t = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_t = self.policy(obs_t)\n",
    "        action_t = action_t.squeeze(0).cpu().numpy()\n",
    "        noisy_action = action_t + eps * self.action_noise()\n",
    "        scaled_action = (\n",
    "            self._action_space.low\n",
    "            + (noisy_action + 1.0)/2.0\n",
    "            * (self._action_space.high - self._action_space.low)\n",
    "        )\n",
    "        return scaled_action\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.buffer.add_transition(transition)\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_noise.reset()\n",
    "\n",
    "    def train(self, iter_fit=32):\n",
    "        if self.buffer.get_size() < self.batch_size:\n",
    "            return []\n",
    "        losses = []\n",
    "        self.train_iter += 1\n",
    "        if self.use_target_net and self.train_iter % self.update_target_every == 0:\n",
    "            self._copy_nets()\n",
    "\n",
    "        for _ in range(iter_fit):\n",
    "            if self.use_per:\n",
    "                (s, a, rew, s_prime, done, weights, idx) = self.buffer.sample(batch_size=self.batch_size)\n",
    "            else:\n",
    "                (s, a, rew, s_prime, done) = self.buffer.sample(batch_size=self.batch_size)\n",
    "                weights, idx = None, None\n",
    "\n",
    "            if self.use_target_net:\n",
    "                q_prime = self.Q_target.Q_value(s_prime, self.policy_target(s_prime))\n",
    "            else:\n",
    "                q_prime = self.Q.Q_value(s_prime, self.policy(s_prime))\n",
    "\n",
    "            td_target = rew + self.discount * (1.0 - done) * q_prime\n",
    "            loss_val, td_err = self.Q.fit(s, a, td_target, weights=weights)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            q_val = self.Q.Q_value(s, self.policy(s))\n",
    "            actor_loss = -torch.mean(q_val if weights is None else q_val * weights)\n",
    "            actor_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.use_per and idx is not None:\n",
    "                td_err = td_err.squeeze(-1)\n",
    "                self.buffer.update_priorities(idx, td_err)\n",
    "\n",
    "            losses.append((loss_val, actor_loss.item()))\n",
    "\n",
    "        self.soft_update()\n",
    "        return losses\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            \"actor\": self.policy.state_dict(),\n",
    "            \"critic\": self.Q.state_dict(),\n",
    "            \"target_actor\": self.policy_target.state_dict(),\n",
    "            \"target_critic\": self.Q_target.state_dict(),\n",
    "            \"actor_optimizer\": self.optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "        self.policy.load_state_dict(data[\"actor\"])\n",
    "        self.Q.load_state_dict(data[\"critic\"])\n",
    "        self.policy_target.load_state_dict(data[\"target_actor\"])\n",
    "        self.Q_target.load_state_dict(data[\"target_critic\"])\n",
    "        self.optimizer.load_state_dict(data[\"actor_optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint_ep500001.pth', 'feedforward.py', 'Hockey_DuelingDQN_train_both_run2_cp3.pt', 'hockey_videos', 'memory.py', 'per_memory.py', 'plots1', 'plotting.ipynb', 'run_client.py', 'Textdokument (neu).txt', 'train_lr_experiment.py', '__pycache__']\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m    \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#env.render()  # visualize the game\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get agent action without noise\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m ddqn \u001b[38;5;241m=\u001b[39m\u001b[43mDDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m a_ddqn \u001b[38;5;241m=\u001b[39m ddqn\u001b[38;5;241m.\u001b[39mget_step(ob)\n\u001b[0;32m     35\u001b[0m a \u001b[38;5;241m=\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mact(ob, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\Downloads\\hockey-ddpg\\ddpg\\run_client.py:199\u001b[0m, in \u001b[0;36mDDQNAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# model_path = \"Hockey_DuelingDQN_finetune_run3_cp12000_eps.pt\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# model_path = \"Hockey_DuelingDQN_finetune_run5_cp22000_eps.pt\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# model_path = \"Hockey_DuelingDQN_finetune_run5_cp200000_eps.pt\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHockey_DuelingDQN_train_both_run2_cp3.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\newRL_env\\Lib\\site-packages\\torch\\serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\newRL_env\\Lib\\site-packages\\torch\\serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\newRL_env\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m     ):\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         )\n\u001b[1;32m--> 512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    514\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\newRL_env\\Lib\\site-packages\\torch\\serialization.py:1928\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1928\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\newRL_env\\Lib\\site-packages\\torch\\serialization.py:1888\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1885\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1888\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1889\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[0;32m   1890\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1891\u001b[0m     )\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from run_client import DDQNAgent\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = hockey.HockeyEnv(mode=\"NORMAL\")\n",
    "ddpg = DDPGAgent(env.observation_space, env.action_space, eps=0.0, learning_rate_actor=0.00001, update_target_every=10)\n",
    "directory = \"./\"\n",
    "print(os.listdir(directory))\n",
    "\n",
    "# Load the checkpoint and restore the agent's state\n",
    "checkpoint = ddpg.load(\"checkpoint_ep500001.pth\")\n",
    "#print(checkpoint.keys())\n",
    "ddpg.reset()\n",
    "\n",
    "\n",
    "# Create the opponent (if needed)\n",
    "player2 = hockey.BasicOpponent(weak=True)\n",
    "\n",
    "# Evaluation loop: run one episode\n",
    "ob, _ = env.reset()\n",
    "done = False\n",
    "win=0\n",
    "matches=0\n",
    "while True:\n",
    "    if matches == 10000:\n",
    "        print(win/matches)\n",
    "        break    \n",
    "    #env.render()  # visualize the game\n",
    "    # Get agent action without noise\n",
    "    \n",
    "    ddqn =DDQNAgent()\n",
    "    a_ddqn = ddqn.get_step(ob)\n",
    "    \n",
    "\n",
    "    \n",
    "    a = ddpg.act(ob, eps=0.0)\n",
    "    # Get opponent's action\n",
    "    a2 = player2.act(env.obs_agent_two())\n",
    "    # Step the environment with the combined actions\n",
    "    ob, reward, done, trunc, info = env.step(np.hstack([a, a2]))\n",
    "    if done:\n",
    "        matches+=1\n",
    "        env.reset()\n",
    "        #if info['winner'] == 1:\n",
    "        win+=max(0,info['winner'])\n",
    "        #print(win/matches)\n",
    "        continue\n",
    "\n",
    "print(win/10000)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
