{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import hockey.hockey_env as h_env\n",
    "import pickle\n",
    "\n",
    "import memory\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mode = True\n",
    "def discrete_to_continous_action(discrete_action):\n",
    "    ''' converts discrete actions into continuous ones (for one player)\n",
    "        The actions allow only one operation each timestep, e.g. X or Y or angle change.\n",
    "        This is surely limiting. Other discrete actions are possible\n",
    "        Action 0: do nothing\n",
    "        Action 1: -1 in x\n",
    "        Action 2: 1 in x\n",
    "        Action 3: -1 in y\n",
    "        Action 4: 1 in y\n",
    "        Action 5: -1 in angle\n",
    "        Action 6: 1 in angle\n",
    "        Action 7: shoot (if keep_mode is on)\n",
    "        '''\n",
    "    action_cont = [(discrete_action == 1) * -1.0 + (discrete_action == 2) * 1.0,  # player x\n",
    "                   (discrete_action == 3) * -1.0 + (discrete_action == 4) * 1.0,  # player y\n",
    "                   (discrete_action == 5) * -1.0 + (discrete_action == 6) * 1.0]  # player angle\n",
    "    if keep_mode:\n",
    "      action_cont.append((discrete_action == 7) * 1.0)\n",
    "\n",
    "    return action_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_copy import QFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent():\n",
    "    def __init__(self,h=None,idx=4) -> None:\n",
    "        if h is None:\n",
    "            h = [1024, 1024, 1024]\n",
    "        self.Q= QFunction(state_dim=18, action_dim=7, hidden_sizes=h, learning_rate=0.001, enable_dueling_dqn=True)\n",
    "        if idx == 4:\n",
    "            model_path = \"models/Hockey_DuelingDQN_finetune_run3_cp12000_eps.pt\"\n",
    "        elif idx == 3:\n",
    "            model_path = \"models/Hockey_DuelingDQN_finetune_run5_cp22000_eps.pt\"\n",
    "        elif idx == 2:\n",
    "            model_path = \"models/Hockey_DuelingDQN_finetune_run5_cp200000_eps.pt\"\n",
    "        elif idx == 1:\n",
    "            model_path = \"models/Hockey_DuelingDQN_train_both_run2_cp3.pt\"\n",
    "        self.Q.load_state_dict(torch.load(model_path))\n",
    "        print(\"Started DDQN agent\")\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.Q.greedy_action(state)\n",
    "\n",
    "    def get_step(self, observation: list[float]) -> list[float]:\n",
    "        action = self.act(np.array(observation)).tolist()\n",
    "        return discrete_to_continous_action(action)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent4_strong = 0\n",
    "eval_agent4_weak = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = h_env.HockeyEnv()\n",
    "ac_space = env.action_space\n",
    "o_space = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started DDQN agent\n",
      "Wins against strong opponent (1): 0.67\n",
      "Wins against weak opponent (1): 0.37\n",
      "Wins against strong opponent (2): 0.66\n",
      "Wins against weak opponent (2): 0.31\n",
      "Wins against strong opponent (3): 0.78\n",
      "Wins against weak opponent (3): 0.43\n",
      "Wins against strong opponent (4): 0.78\n",
      "Wins against weak opponent (4): 0.43\n"
     ]
    }
   ],
   "source": [
    "eval_eps = 100\n",
    "agent = DDQNAgent()\n",
    "opponent = h_env.BasicOpponent(weak=False)\n",
    "wins = 0\n",
    "\n",
    "for i in range(eval_eps):\n",
    "    state, _info = env.reset()\n",
    "    for t in range(500):\n",
    "        done = False\n",
    "        a_discrete = agent.act(state)\n",
    "        a_opponent = opponent.act(env.obs_agent_two())\n",
    "        a = discrete_to_continous_action(a_discrete)\n",
    "        state, reward, done, trunc, info = env.step(np.hstack([a, a_opponent]))\n",
    "        if done: break\n",
    "    if info[\"winner\"] == 1:\n",
    "        wins += 1\n",
    "\n",
    "wins /= eval_eps\n",
    "eval_agent4_strong = wins\n",
    "\n",
    "opponent = h_env.BasicOpponent(weak=True)\n",
    "wins = 0\n",
    "\n",
    "for i in range(eval_eps):\n",
    "    state, _info = env.reset()\n",
    "    for t in range(500):\n",
    "        done = False\n",
    "        a_discrete = agent.act(state)\n",
    "        a_opponent = opponent.act(state)\n",
    "        a = discrete_to_continous_action(a_discrete)\n",
    "        state, reward, done, trunc, info = env.step(np.hstack([a, a_opponent]))\n",
    "        if done: break\n",
    "    if info[\"winner\"] == 1:\n",
    "        wins += 1\n",
    "\n",
    "wins /= eval_eps\n",
    "eval_agent4_weak = wins\n",
    "\n",
    "print(f\"Wins against strong opponent (1): {eval_agent1_strong}\")\n",
    "print(f\"Wins against weak opponent (1): {eval_agent1_weak}\")\n",
    "print(f\"Wins against strong opponent (2): {eval_agent2_strong}\")\n",
    "print(f\"Wins against weak opponent (2): {eval_agent2_weak}\")\n",
    "print(f\"Wins against strong opponent (3): {eval_agent3_strong}\")\n",
    "print(f\"Wins against weak opponent (3): {eval_agent3_weak}\")\n",
    "print(f\"Wins against strong opponent (4): {eval_agent3_strong}\")\n",
    "print(f\"Wins against weak opponent (4): {eval_agent3_weak}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started DDQN agent\n",
      "Started DDQN agent\n",
      "Started DDQN agent\n",
      "Started DDQN agent\n",
      "Match: agent 1 vs 2 ended with 0.285 : 0.7150000000000001\n",
      "Match: agent 1 vs 3 ended with 0.28 : 0.72\n",
      "Match: agent 1 vs 4 ended with 0.26 : 0.74\n",
      "Match: agent 2 vs 3 ended with 0.3 : 0.7\n",
      "Match: agent 2 vs 4 ended with 0.33 : 0.6699999999999999\n",
      "Match: agent 3 vs 4 ended with 0.265 : 0.735\n"
     ]
    }
   ],
   "source": [
    "agent1 = DDQNAgent(h=[256,256,256],idx=1)\n",
    "agent2 = DDQNAgent(idx=2)\n",
    "agent3 = DDQNAgent(idx=3)\n",
    "agent4 = DDQNAgent(idx=4)\n",
    "\n",
    "pairs = [(agent1, agent2), (agent1, agent3), (agent1, agent4), (agent2, agent3), (agent2, agent4), (agent3, agent4)]\n",
    "names = [\"agent 1 vs 2\", \"agent 1 vs 3\", \"agent 1 vs 4\", \"agent 2 vs 3\", \"agent 2 vs 4\", \"agent 3 vs 4\"]\n",
    "stats = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "    agent = pairs[i][0]\n",
    "    opponent = pairs[i][1]\n",
    "    wins = 0\n",
    "    for _ in range(eval_eps):\n",
    "        state, _info = env.reset()\n",
    "        for t in range(500):\n",
    "            done = False\n",
    "            a_discrete = agent.act(state)\n",
    "            a_opponent = discrete_to_continous_action(opponent.act(env.obs_agent_two()))\n",
    "            a = discrete_to_continous_action(a_discrete)\n",
    "            state, reward, done, trunc, info = env.step(np.hstack([a, a_opponent]))\n",
    "            if done: break\n",
    "        if info[\"winner\"] == 1:\n",
    "            wins += 1\n",
    "\n",
    "    for _ in range(eval_eps):\n",
    "        state, _info = env.reset()\n",
    "        for t in range(500):\n",
    "            done = False\n",
    "            a_discrete = agent.act(env.obs_agent_two())\n",
    "            a_opponent = discrete_to_continous_action(opponent.act(state))\n",
    "            a = discrete_to_continous_action(a_discrete)\n",
    "            state, reward, done, trunc, info = env.step(np.hstack([ a_opponent, a]))\n",
    "            if done: break\n",
    "        if info[\"winner\"] == 0:\n",
    "            wins += 1\n",
    "\n",
    "    stats[i] = wins / (2.0 * eval_eps)\n",
    "    print(f\"Match: {names[i]} ended with {stats[i]} : {1 - stats[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "['agent 1 vs 2', 'agent 1 vs 3', 'agent 1 vs 4', 'agent 2 vs 3', 'agent 2 vs 4', 'agent 3 vs 4']\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(stats)\n",
    "print(names)\n",
    "print(len(stats))\n",
    "print(len(names))\n",
    "print(len(pairs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
