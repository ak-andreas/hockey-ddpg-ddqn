{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoceky Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import hockey.hockey_env as h_env\n",
    "import pickle\n",
    "\n",
    "import memory\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "implemets Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: torch.autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else torch.autograd.Variable(*args, **kwargs)\n",
    "class NoisyLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        self.weight_mu = torch.nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = torch.nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = torch.nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = torch.nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(Variable(self.weight_epsilon))\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(Variable(self.bias_epsilon))\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.weight_mu.size(1))\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.weight_sigma.size(1)))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, enable_dueling_dqn=False, use_noise=False, device = 'cpu'):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.enable_dueling_dqn = enable_dueling_dqn\n",
    "        self.device = device\n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            # cut the last hidden layer to the advantage and value streams\n",
    "            self.dueling_size = self.hidden_sizes[-1]\n",
    "            self.hidden_sizes = self.hidden_sizes[:-1]\n",
    "\n",
    "        layers = []\n",
    "        in_size = self.input_size\n",
    "        first = True\n",
    "        for h in hidden_sizes:\n",
    "            if first or not use_noise:\n",
    "                layers.append(torch.nn.Linear(in_size, h))\n",
    "                first = False\n",
    "            else:\n",
    "                layers.append(NoisyLinear(in_size, h))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            in_size = h\n",
    "        \n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            if use_noise:\n",
    "                # Value stream\n",
    "                self.fc_value = NoisyLinear(in_size, self.dueling_size)\n",
    "                self.value = NoisyLinear(self.dueling_size, 1)\n",
    "\n",
    "                # Advantages stream\n",
    "                self.fc_advantages = NoisyLinear(in_size, self.dueling_size)\n",
    "                self.advantages = NoisyLinear(self.dueling_size, self.output_size)\n",
    "            else:\n",
    "                # Value stream\n",
    "                self.fc_value = torch.nn.Linear(in_size, self.dueling_size)\n",
    "                self.value = torch.nn.Linear(self.dueling_size, 1)\n",
    "\n",
    "                # Advantages stream\n",
    "                self.fc_advantages = torch.nn.Linear(in_size, self.dueling_size)\n",
    "                self.advantages = torch.nn.Linear(self.dueling_size, self.output_size)\n",
    "        else:\n",
    "            if use_noise:\n",
    "                layers.append(NoisyLinear(in_size, output_size))\n",
    "            else:\n",
    "                layers.append(torch.nn.Linear(in_size, output_size))\n",
    "\n",
    "        self.fully_connected = torch.nn.Sequential(*layers)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Returns [batch_size, action_space_size]\n",
    "        '''\n",
    "        x = self.fully_connected(x)\n",
    "        if self.enable_dueling_dqn:\n",
    "            # Value calculation\n",
    "            v = F.relu(self.fc_value(x))\n",
    "            V = self.value(v)\n",
    "\n",
    "            # Advantages calculation\n",
    "            a = F.relu(self.fc_advantages(x))\n",
    "            A = self.advantages(a)\n",
    "\n",
    "            # Calculate Q\n",
    "            Q = V + A - torch.mean(A, dim=-1, keepdim=True)\n",
    "        \n",
    "        else:\n",
    "            Q = x\n",
    "\n",
    "        return Q\n",
    "            \n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Runs without gradients and takes and returns numpy arrays\n",
    "        '''\n",
    "        x = torch.from_numpy(x).float().to(self.device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x).cpu().numpy()\n",
    "        self.train()\n",
    "        return out\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        for layer in self.fully_connected:\n",
    "            if isinstance(layer, NoisyLinear):\n",
    "                layer.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Function\n",
    "Uses the network to provide a function that can be used as Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(Feedforward):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes, learning_rate, enable_dueling_dqn=False, use_noise=False, device = 'cpu'):\n",
    "        super().__init__(input_size=state_dim,\n",
    "                         hidden_sizes=hidden_sizes,\n",
    "                         output_size=action_dim,\n",
    "                         enable_dueling_dqn=enable_dueling_dqn,\n",
    "                         use_noise=use_noise,\n",
    "                         device=device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss = torch.nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "    def fit(self, states, actions, targets, weights):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        acts = torch.from_numpy(actions).to(self.device)\n",
    "        pred = self.Q_value(torch.from_numpy(states).float().to(self.device), acts)\n",
    "        if weights is None:\n",
    "            weights = torch.ones_like(pred)\n",
    "        weights = weights.to(self.device)\n",
    "        loss = (weights * self.loss(pred, torch.from_numpy(targets).float().to(self.device))).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        td_error = pred.detach().cpu().numpy() - targets\n",
    "        return loss.item(), td_error\n",
    "    \n",
    "    def Q_value(self, states, actions):\n",
    "        return self.forward(states).gather(1, actions[:, None])\n",
    "    \n",
    "    def maxQ(self, states):\n",
    "        return np.max(self.predict(states), axis=-1, keepdims=True)\n",
    "    \n",
    "    def doubleQ(self, state, action):\n",
    "        x = torch.from_numpy(state).float().to(self.device)\n",
    "        action = torch.from_numpy(action).to(self.device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            Q = self.Q_value(x, action)\n",
    "        return Q.cpu().numpy()\n",
    "\n",
    "    \n",
    "    def greedy_action(self, states):\n",
    "        return np.argmax(self.predict(states), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "uses target net, epsilon-decay, double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.config = config\n",
    "        \n",
    "        self.eps = self.config['eps']\n",
    "        self.eps_decay = self.config['eps_decay']\n",
    "        self.eps_min = self.config['eps_min']\n",
    "        self.iter_fit = self.config['iter_fit']\n",
    "        self.use_noise = self.config['enable_noisy_nets']\n",
    "\n",
    "        if self.config[\"enable_prioritized_replay\"]:\n",
    "            self.buffer = memory.PrioritizedMemory(max_size=self.config['buffer_size'], \n",
    "                                                   alpha=self.config['alpha'],\n",
    "                                                   beta=self.config['beta'])\n",
    "        else:\n",
    "            self.buffer = memory.Memory(self.config['buffer_size'])\n",
    "\n",
    "        self.Q = QFunction(state_dim=self.observation_space.shape[0],\n",
    "                           action_dim=self.action_space.n,\n",
    "                           hidden_sizes=self.config['hidden_sizes'],\n",
    "                           learning_rate=self.config['learning_rate'],\n",
    "                           enable_dueling_dqn=self.config['enable_dueling_dqn'],\n",
    "                           use_noise=self.config['enable_noisy_nets'],\n",
    "                           device=self.config['device'])\n",
    "        self.Q_target = QFunction(state_dim=self.observation_space.shape[0], \n",
    "                                  action_dim=self.action_space.n,\n",
    "                                  hidden_sizes=self.config['hidden_sizes'],\n",
    "                                  learning_rate=0,\n",
    "                                  enable_dueling_dqn=self.config['enable_dueling_dqn'],\n",
    "                                  use_noise=self.config['enable_noisy_nets'],\n",
    "                                  device=self.config['device'])\n",
    "        \n",
    "        self.update_target()\n",
    "        self.train_iter = 0\n",
    "\n",
    "    def update_target(self):\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "    \n",
    "    def act(self, state, eps=None):\n",
    "        if self.use_noise:\n",
    "            return self.Q.greedy_action(state)\n",
    "        else: \n",
    "            if eps is None:\n",
    "                eps = self.eps\n",
    "            if np.random.rand() < eps:\n",
    "                return self.action_space.sample()\n",
    "            else:\n",
    "                return self.Q.greedy_action(state)\n",
    "        \n",
    "    # def act_safe(self, state, eps=None):\n",
    "    #     return __env.discrete_to_continous_action(self.act(state, eps))\n",
    "        \n",
    "    def store_transition(self, transition):\n",
    "        self.buffer.add_transition(transition)\n",
    "\n",
    "    def train(self):\n",
    "        losses = []\n",
    "        self.train_iter += 1\n",
    "        # Update target network if needed\n",
    "        if self.config[\"use_target_net\"] and self.train_iter % self.config[\"update_target_every\"] == 0:\n",
    "            self.update_target()\n",
    "\n",
    "        # train with given buffer for (iter_fit) mini-batches\n",
    "        for _ in range(self.iter_fit):\n",
    "            # smaple from buffer\n",
    "            if self.config[\"enable_prioritized_replay\"]:\n",
    "                data, indices, weights = self.buffer.sample(batch=self.config['batch_size'])\n",
    "            else:\n",
    "                data = self.buffer.sample(batch=self.config['batch_size'])\n",
    "                weights = None\n",
    "\n",
    "            # extract batches of every element -> [32, 1] for rewards\n",
    "            s = np.stack(data[:, 0])\n",
    "            a = np.stack(data[:, 1])\n",
    "            rew = np.stack(data[:, 2])[:, None]\n",
    "            s_prime = np.stack(data[:, 3])\n",
    "            done = np.stack(data[:, 4])[:, None]\n",
    "\n",
    "            if self.config[\"use_target_net\"]:\n",
    "                if self.config[\"enable_double_dqn\"]:\n",
    "                    best_action = self.Q.greedy_action(s_prime)\n",
    "                    v_prime = self.Q_target.doubleQ(s_prime, best_action)\n",
    "                else:\n",
    "                    v_prime = self.Q_target.maxQ(s_prime)\n",
    "            else:\n",
    "                v_prime = self.Q.maxQ(s_prime)\n",
    "            gamma = self.config[\"discount\"]\n",
    "            td_target = rew + gamma * (1 - done) * v_prime\n",
    "\n",
    "            fit_loss, td_error = self.Q.fit(s, a, td_target, weights)\n",
    "            if self.config[\"enable_prioritized_replay\"]:\n",
    "                self.buffer.update_priorities(indices, td_error)\n",
    "            losses.append(fit_loss)\n",
    "            \n",
    "            self.Q.reset_noise()\n",
    "            self.Q_target.reset_noise()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.eps = max(self.eps_min, self.eps * self.eps_decay)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n",
      "Box(-inf, inf, (18,), float32)\n",
      "[(np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf)), (np.float32(-inf), np.float32(inf))]\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "env = h_env.HockeyEnv()\n",
    "\n",
    "ac_space = env.discrete_action_space\n",
    "o_space = env.observation_space\n",
    "print(ac_space)\n",
    "print(o_space)\n",
    "print(list(zip(env.observation_space.low, env.observation_space.high)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continous_action(discrete_action):\n",
    "    \"\"\"\n",
    "    x, y, angle, shoot ()\n",
    "\n",
    "    Action 0: do nothing\n",
    "    Action 1: x = 1\n",
    "    Action 2: x = 1, y = 1\n",
    "    Action 3: x = 1, y = 1, angle = 1\n",
    "    Action 4: x = 1, y = 1, angle = -1\n",
    "    Action 5: x = 1, y = -1\n",
    "    Action 6: x = 1, y = -1, angle = 1\n",
    "    Action 7: x = 1, y = -1, angle = -1\n",
    "    Action 8: x = -1\n",
    "    Action 9: x = -1, y = 1\n",
    "    Action 10: x = -1, y = 1, angle = 1\n",
    "    Action 11: x = -1, y = 1, angle = -1\n",
    "    Action 12: x = -1, y = -1\n",
    "    Action 13: x = -1, y = -1, angle = 1\n",
    "    Action 14: x = -1, y = -1, angle = -1\n",
    "    Action 15: shoot\n",
    "    \"\"\"\n",
    "    x = (discrete_action < 8 and discrete_action > 1) * 1.0 + (discrete_action > 7 and discrete_action < 15) * -1.0\n",
    "    y = (any([discrete_action == n for n in [2,3,4,9,10,11]]) * 1.0 + any([discrete_action == n for n in [5,6,7,12,13,14]]) * -1.0)\n",
    "    angle = (any([discrete_action == n for n in [3,6,10,13]]) * 1.0 + any([discrete_action == n for n in [4,7,11,14]]) * -1.0)\n",
    "\n",
    "    action_cont = [x, y, angle, discrete_action == 15]\n",
    "    return action_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "ac_space = gym.spaces.Discrete(16)\n",
    "print(ac_space.n)\n",
    "ac_space = env.discrete_action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_sizes\": [1024, 1024, 1024],\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 128,\n",
    "    \"iter_fit\": 32,\n",
    "    \"max_episodes\": 50000,\n",
    "    \"max_steps\": 500,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"discount\": 0.95,\n",
    "    \"use_target_net\": True,\n",
    "    \"update_target_every\": 20,\n",
    "    \"enable_dueling_dqn\": True,\n",
    "    \"enable_double_dqn\": True,\n",
    "    \"enable_prioritized_replay\": True,\n",
    "    \"alpha\": 0.6,\n",
    "    \"beta\": 0.4,\n",
    "    \"enable_noisy_nets\": False,\n",
    "    \"eps\": 0.7,\n",
    "    \"eps_decay\": 0.9995,\n",
    "    \"eps_min\": 0.05,\n",
    "    \"weak_percent\": 0.2,\n",
    "    \"self_percent\": 0.2,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"eval_every\": 200,\n",
    "    \"eval_episodes\": 100,\n",
    "    \"print_every\": 20\n",
    "}\n",
    "env_name = \"Hockey\"\n",
    "run_name = f\"{env_name}_Rainbow_run{7}\"\n",
    "\n",
    "opponent_weak = h_env.BasicOpponent(weak=True)\n",
    "opponent_strong = h_env.BasicOpponent(weak=False)\n",
    "\n",
    "agent = DQNAgent(o_space, ac_space, config)\n",
    "writer = SummaryWriter(log_dir=f\"logs/{run_name}\")\n",
    "\n",
    "checkpoint_path = \"models/checkpoints/\" + run_name + \"_cp\"\n",
    "checkpoint_idx = 0\n",
    "checkpoint_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Done after 131 steps. Reward: 7.522185312412419\n",
      "Evaluation after 0 episodes: -24.518816899995723 took 34.90857124328613s\n",
      "New best model with nan% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp0.pt\n",
      "20: Done after 251 steps. Reward: -1.2577178637926212\n",
      "40: Done after 251 steps. Reward: -40.420782582031066\n",
      "60: Done after 251 steps. Reward: -20.848712847330248\n",
      "80: Done after 14 steps. Reward: 10.0\n",
      "100: Done after 251 steps. Reward: -5.131349473553475\n",
      "120: Done after 43 steps. Reward: 9.061815536126204\n",
      "140: Done after 55 steps. Reward: -13.842254447436938\n",
      "160: Done after 48 steps. Reward: -12.073959571127649\n",
      "180: Done after 251 steps. Reward: -23.537512398702116\n",
      "200: Done after 251 steps. Reward: -31.034378010863758\n",
      "Evaluation after 200 episodes: -17.903289053447658 took 34.92914366722107s\n",
      "New best model with 270.00% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp1.pt\n",
      "220: Done after 251 steps. Reward: -31.94324833785695\n",
      "240: Done after 163 steps. Reward: 8.95847967562782\n",
      "260: Done after 251 steps. Reward: -14.866079628024773\n",
      "280: Done after 37 steps. Reward: 9.373782711409884\n",
      "300: Done after 75 steps. Reward: -13.328340870306375\n",
      "320: Done after 251 steps. Reward: -43.01809738542567\n",
      "340: Done after 251 steps. Reward: -20.17076634892361\n",
      "360: Done after 251 steps. Reward: -29.275662413833068\n",
      "380: Done after 68 steps. Reward: -15.782962616965824\n",
      "400: Done after 35 steps. Reward: 9.424161647542075\n",
      "Evaluation after 400 episodes: -16.32657232119856 took 35.257105350494385s\n",
      "New best model with 10.81% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp2.pt\n",
      "420: Done after 92 steps. Reward: -15.08656514190507\n",
      "440: Done after 82 steps. Reward: -15.218933289065712\n",
      "460: Done after 251 steps. Reward: -16.55084610549607\n",
      "480: Done after 251 steps. Reward: -21.80838973498019\n",
      "500: Done after 138 steps. Reward: -15.096011140255893\n",
      "520: Done after 50 steps. Reward: -12.442920444962013\n",
      "540: Done after 251 steps. Reward: -19.895254228057414\n",
      "560: Done after 52 steps. Reward: -13.928683588927868\n",
      "580: Done after 63 steps. Reward: -12.681970899179802\n",
      "600: Done after 96 steps. Reward: -14.108938564409529\n",
      "Evaluation after 600 episodes: -15.883645440090449 took 34.05865478515625s\n",
      "620: Done after 251 steps. Reward: -23.747457017141294\n",
      "640: Done after 162 steps. Reward: -19.995046324499732\n",
      "660: Done after 251 steps. Reward: -7.066353305091059\n",
      "680: Done after 54 steps. Reward: -13.379592591294129\n",
      "700: Done after 251 steps. Reward: -13.382902294858466\n",
      "720: Done after 92 steps. Reward: -14.237715392264183\n",
      "740: Done after 251 steps. Reward: -21.41183223497816\n",
      "760: Done after 38 steps. Reward: 9.285941946197386\n",
      "780: Done after 196 steps. Reward: -13.366520695418867\n",
      "800: Done after 119 steps. Reward: -13.688244081420523\n",
      "Evaluation after 800 episodes: -20.56346419753855 took 34.25108194351196s\n",
      "820: Done after 213 steps. Reward: -12.384434430309101\n",
      "840: Done after 98 steps. Reward: -16.318525969300552\n",
      "860: Done after 51 steps. Reward: -13.00045880627108\n",
      "880: Done after 37 steps. Reward: 9.238849295990832\n",
      "900: Done after 65 steps. Reward: -14.159849233001582\n",
      "920: Done after 221 steps. Reward: -21.060674521376654\n",
      "940: Done after 78 steps. Reward: -13.488723616286096\n",
      "960: Done after 49 steps. Reward: -12.979955438822063\n",
      "980: Done after 251 steps. Reward: -0.7345871711559459\n",
      "1000: Done after 251 steps. Reward: -10.300893647960246\n",
      "Evaluation after 1000 episodes: -20.34506262860156 took 38.36940932273865s\n",
      "1020: Done after 232 steps. Reward: 4.535324471620997\n",
      "1040: Done after 61 steps. Reward: 8.976514708116802\n",
      "1060: Done after 37 steps. Reward: 8.892522334419064\n",
      "1080: Done after 251 steps. Reward: -10.568169304857484\n",
      "1100: Done after 176 steps. Reward: -20.599565589717756\n",
      "1120: Done after 40 steps. Reward: 9.238111943332711\n",
      "1140: Done after 168 steps. Reward: -14.892426830833092\n",
      "1160: Done after 84 steps. Reward: -13.449764402945974\n",
      "1180: Done after 92 steps. Reward: 6.883265369349715\n",
      "1200: Done after 177 steps. Reward: -18.429695953480334\n",
      "Evaluation after 1200 episodes: -21.491878049449312 took 36.56471538543701s\n",
      "1220: Done after 85 steps. Reward: -12.76916863930344\n",
      "1240: Done after 90 steps. Reward: -12.435202564246016\n",
      "1260: Done after 39 steps. Reward: 9.3494095665706\n",
      "1280: Done after 251 steps. Reward: -0.39074317134453307\n",
      "1300: Done after 46 steps. Reward: 8.741133311945413\n",
      "1320: Done after 155 steps. Reward: -12.372332589016288\n",
      "1340: Done after 251 steps. Reward: -3.432206880339865\n",
      "1360: Done after 251 steps. Reward: -4.674548268771458\n",
      "1380: Done after 251 steps. Reward: -9.681460368467464\n",
      "1400: Done after 74 steps. Reward: 9.288608303565772\n",
      "Evaluation after 1400 episodes: -21.427487325628082 took 37.64942765235901s\n",
      "1420: Done after 251 steps. Reward: -5.6819295335216085\n",
      "1440: Done after 58 steps. Reward: -12.772013028012264\n",
      "1460: Done after 110 steps. Reward: -16.1168366297166\n",
      "1480: Done after 120 steps. Reward: -20.54907545955087\n",
      "1500: Done after 114 steps. Reward: -14.439431626988956\n",
      "1520: Done after 39 steps. Reward: -11.487744270395364\n",
      "1540: Done after 115 steps. Reward: -15.334803162375785\n",
      "1560: Done after 50 steps. Reward: -12.754771041557404\n",
      "1580: Done after 65 steps. Reward: 8.683974649036262\n",
      "1600: Done after 64 steps. Reward: -12.576021381605088\n",
      "Evaluation after 1600 episodes: -18.14325650932259 took 30.668172597885132s\n",
      "1620: Done after 147 steps. Reward: 5.895297145000317\n",
      "1640: Done after 53 steps. Reward: -12.34222434592369\n",
      "1660: Done after 104 steps. Reward: 8.279732024188501\n",
      "1680: Done after 251 steps. Reward: -10.902025695282328\n",
      "1700: Done after 251 steps. Reward: -8.64105307676529\n",
      "1720: Done after 34 steps. Reward: 9.11733534822308\n",
      "1740: Done after 51 steps. Reward: -13.467868508180432\n",
      "1760: Done after 180 steps. Reward: -16.739487232538917\n",
      "1780: Done after 53 steps. Reward: -14.018629127415654\n",
      "1800: Done after 48 steps. Reward: 9.306243766487817\n",
      "Evaluation after 1800 episodes: -21.877005302879624 took 35.90076494216919s\n",
      "1820: Done after 38 steps. Reward: 9.340846874196194\n",
      "1840: Done after 145 steps. Reward: -14.933392047032854\n",
      "1860: Done after 92 steps. Reward: -13.958206747521867\n",
      "1880: Done after 139 steps. Reward: -15.732798536074224\n",
      "1900: Done after 251 steps. Reward: -17.743961346858793\n",
      "1920: Done after 121 steps. Reward: 8.928240810217673\n",
      "1940: Done after 49 steps. Reward: -12.756143904919696\n",
      "1960: Done after 126 steps. Reward: -16.73533782184785\n",
      "1980: Done after 251 steps. Reward: -15.397162618217525\n",
      "2000: Done after 50 steps. Reward: 9.329393242838721\n",
      "Evaluation after 2000 episodes: -21.969148594604288 took 34.77936005592346s\n",
      "2020: Done after 107 steps. Reward: -16.35963846813467\n",
      "2040: Done after 228 steps. Reward: -15.553155098500199\n",
      "2060: Done after 243 steps. Reward: 6.273880245337338\n",
      "2080: Done after 41 steps. Reward: 9.01168077574033\n",
      "2100: Done after 251 steps. Reward: -3.0439711079444147\n",
      "2120: Done after 35 steps. Reward: -11.039396168244997\n",
      "2140: Done after 73 steps. Reward: -14.44026424758626\n",
      "2160: Done after 201 steps. Reward: 4.779624390253012\n",
      "2180: Done after 116 steps. Reward: 9.262390968017872\n",
      "2200: Done after 43 steps. Reward: 8.922407059758873\n",
      "Evaluation after 2200 episodes: -21.111146806060425 took 33.038456201553345s\n",
      "2220: Done after 104 steps. Reward: 7.009485952835446\n",
      "2240: Done after 30 steps. Reward: 9.503180848389773\n",
      "2260: Done after 93 steps. Reward: -13.210997107002965\n",
      "2280: Done after 94 steps. Reward: 9.306921196982678\n",
      "2300: Done after 38 steps. Reward: 9.538579624019818\n",
      "2320: Done after 216 steps. Reward: -14.871558954731816\n",
      "2340: Done after 83 steps. Reward: -14.26259776863383\n",
      "2360: Done after 30 steps. Reward: 9.501877287755322\n",
      "2380: Done after 62 steps. Reward: -12.661612065693765\n",
      "2400: Done after 59 steps. Reward: -12.819642901701389\n",
      "Evaluation after 2400 episodes: -17.637354968399187 took 35.880523443222046s\n",
      "New best model with 4.88% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp0.pt\n",
      "2420: Done after 251 steps. Reward: -18.459226214853604\n",
      "2440: Done after 251 steps. Reward: -8.608534226358296\n",
      "2460: Done after 225 steps. Reward: -14.68731252040224\n",
      "2480: Done after 43 steps. Reward: -12.046458467670845\n",
      "2500: Done after 251 steps. Reward: -12.404747579781118\n",
      "2520: Done after 120 steps. Reward: -15.638774476071694\n",
      "2540: Done after 99 steps. Reward: -13.171201707283924\n",
      "2560: Done after 251 steps. Reward: -8.85504116111415\n",
      "2580: Done after 188 steps. Reward: 5.087599563972622\n",
      "2600: Done after 35 steps. Reward: 9.563723525731518\n",
      "Evaluation after 2600 episodes: -19.365456238522203 took 33.60120749473572s\n",
      "2620: Done after 141 steps. Reward: 6.403399282987398\n",
      "2640: Done after 39 steps. Reward: 9.169387959172074\n",
      "2660: Done after 251 steps. Reward: -2.1949865697752573\n",
      "2680: Done after 40 steps. Reward: -11.67917848178172\n",
      "2700: Done after 95 steps. Reward: -14.680292533276697\n",
      "2720: Done after 71 steps. Reward: 9.29403667396224\n",
      "2740: Done after 84 steps. Reward: 8.788307926138929\n",
      "2760: Done after 194 steps. Reward: -13.840901010450477\n",
      "2780: Done after 43 steps. Reward: -11.898542528246777\n",
      "2800: Done after 200 steps. Reward: 5.763945816359439\n",
      "Evaluation after 2800 episodes: -19.077418109663757 took 32.46157360076904s\n",
      "2820: Done after 47 steps. Reward: -11.848371435662887\n",
      "2840: Done after 59 steps. Reward: -14.851130654602489\n",
      "2860: Done after 80 steps. Reward: -14.502309618813214\n",
      "2880: Done after 213 steps. Reward: -17.640248627300192\n",
      "2900: Done after 251 steps. Reward: -15.607694510651834\n",
      "2920: Done after 54 steps. Reward: -13.298635198828501\n",
      "2940: Done after 104 steps. Reward: -13.510689034931625\n",
      "2960: Done after 157 steps. Reward: -14.229117052382797\n",
      "2980: Done after 124 steps. Reward: -15.33751971617302\n",
      "3000: Done after 251 steps. Reward: -15.424021401245911\n",
      "Evaluation after 3000 episodes: -16.349743107289964 took 34.5436851978302s\n",
      "3020: Done after 35 steps. Reward: 9.643175668147006\n",
      "3040: Done after 251 steps. Reward: -3.1160128406645677\n",
      "3060: Done after 81 steps. Reward: -11.576983430380466\n",
      "3080: Done after 166 steps. Reward: -14.080884942163735\n",
      "3100: Done after 155 steps. Reward: -13.874789980999076\n",
      "3120: Done after 251 steps. Reward: -4.1526000662377855\n",
      "3140: Done after 133 steps. Reward: 6.994218698523987\n",
      "3160: Done after 74 steps. Reward: 9.330831875100216\n",
      "3180: Done after 89 steps. Reward: -14.460080014958859\n",
      "3200: Done after 251 steps. Reward: -14.023564110949572\n",
      "Evaluation after 3200 episodes: -21.719578885075276 took 35.04982590675354s\n",
      "3220: Done after 33 steps. Reward: 9.251672671694104\n",
      "3240: Done after 52 steps. Reward: -12.391416332393664\n",
      "3260: Done after 57 steps. Reward: -13.960999881556983\n",
      "3280: Done after 163 steps. Reward: -14.441297308301783\n",
      "3300: Done after 32 steps. Reward: 9.663395849172987\n",
      "3320: Done after 37 steps. Reward: 9.484032056078053\n",
      "3340: Done after 244 steps. Reward: 5.755091763380026\n",
      "3360: Done after 251 steps. Reward: -1.2690472485371247\n",
      "3380: Done after 251 steps. Reward: -6.945753355857198\n",
      "3400: Done after 34 steps. Reward: 9.255906531071007\n",
      "Evaluation after 3400 episodes: -19.799275064306865 took 33.53254461288452s\n",
      "3420: Done after 110 steps. Reward: 7.9663710721937395\n",
      "3440: Done after 251 steps. Reward: -16.783022640100764\n",
      "3460: Done after 64 steps. Reward: 9.272306244543694\n",
      "3480: Done after 251 steps. Reward: -11.765515938312769\n",
      "3500: Done after 251 steps. Reward: -7.703234230714638\n",
      "3520: Done after 152 steps. Reward: 9.279790562176787\n",
      "3540: Done after 206 steps. Reward: -17.021901407908793\n",
      "3560: Done after 52 steps. Reward: -12.355557758221925\n",
      "3580: Done after 117 steps. Reward: -14.503235473650818\n",
      "3600: Done after 222 steps. Reward: 1.16102153129032\n",
      "Evaluation after 3600 episodes: -15.803694586385582 took 32.82169818878174s\n",
      "3620: Done after 251 steps. Reward: -10.704716449670123\n",
      "3640: Done after 37 steps. Reward: 9.497511964869624\n",
      "3660: Done after 251 steps. Reward: -0.8302816208147379\n",
      "3680: Done after 42 steps. Reward: 8.995015371055917\n",
      "3700: Done after 251 steps. Reward: -0.7939216992335596\n",
      "3720: Done after 31 steps. Reward: 9.408907556522243\n",
      "3740: Done after 155 steps. Reward: -14.752501779635882\n",
      "3760: Done after 124 steps. Reward: 6.624307894307368\n",
      "3780: Done after 52 steps. Reward: -12.302989841381935\n",
      "3800: Done after 34 steps. Reward: 9.349398991134644\n",
      "Evaluation after 3800 episodes: -15.638709645915572 took 34.14595699310303s\n",
      "3820: Done after 137 steps. Reward: -15.093135432888207\n",
      "3840: Done after 62 steps. Reward: -12.614393029146111\n",
      "3860: Done after 251 steps. Reward: -14.051915777174699\n",
      "3880: Done after 251 steps. Reward: -6.188603198503081\n",
      "3900: Done after 70 steps. Reward: 9.252057753035114\n",
      "3920: Done after 183 steps. Reward: 9.42806935577019\n",
      "3940: Done after 51 steps. Reward: -12.657353899526754\n",
      "3960: Done after 251 steps. Reward: -7.314143828554416\n",
      "3980: Done after 71 steps. Reward: -11.893687584657453\n",
      "4000: Done after 192 steps. Reward: -18.374202136947602\n",
      "Evaluation after 4000 episodes: -16.532727867420178 took 32.503501892089844s\n",
      "4020: Done after 93 steps. Reward: 8.904724971005589\n",
      "4040: Done after 251 steps. Reward: -11.494773940765558\n",
      "4060: Done after 37 steps. Reward: 9.126904109578081\n",
      "4080: Done after 120 steps. Reward: -13.433852321479366\n",
      "4100: Done after 251 steps. Reward: -7.57912438757309\n",
      "4120: Done after 40 steps. Reward: 8.869068126022244\n",
      "4140: Done after 68 steps. Reward: -13.437220712648399\n",
      "4160: Done after 30 steps. Reward: 9.584883019510032\n",
      "4180: Done after 227 steps. Reward: -18.690434404853463\n",
      "4200: Done after 28 steps. Reward: 9.634172060293958\n",
      "Evaluation after 4200 episodes: -14.616920471143994 took 34.64413785934448s\n",
      "New best model with 20.93% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp1.pt\n",
      "4220: Done after 51 steps. Reward: -13.113024636518244\n",
      "4240: Done after 94 steps. Reward: -16.48393056152423\n",
      "4260: Done after 35 steps. Reward: 9.721676159331553\n",
      "4280: Done after 251 steps. Reward: -7.385397320480873\n",
      "4300: Done after 88 steps. Reward: 7.289451588789271\n",
      "4320: Done after 52 steps. Reward: -12.51437499911128\n",
      "4340: Done after 52 steps. Reward: -12.339962520860789\n",
      "4360: Done after 124 steps. Reward: 8.398348328699885\n",
      "4380: Done after 64 steps. Reward: 8.505589510979704\n",
      "4400: Done after 39 steps. Reward: 9.134331987945288\n",
      "Evaluation after 4400 episodes: -15.17660087926236 took 35.083221435546875s\n",
      "4420: Done after 164 steps. Reward: -20.1219754682781\n",
      "4440: Done after 62 steps. Reward: 9.44802151110589\n",
      "4460: Done after 58 steps. Reward: -12.291894821612061\n",
      "4480: Done after 140 steps. Reward: 7.214093249146764\n",
      "4500: Done after 251 steps. Reward: -16.3003330002997\n",
      "4520: Done after 251 steps. Reward: -7.96432137855287\n",
      "4540: Done after 40 steps. Reward: -11.4558871395127\n",
      "4560: Done after 41 steps. Reward: 9.021690471403677\n",
      "4580: Done after 251 steps. Reward: -5.744373338625578\n",
      "4600: Done after 187 steps. Reward: -16.371692321986913\n",
      "Evaluation after 4600 episodes: -16.039124109751228 took 31.979568481445312s\n",
      "4620: Done after 162 steps. Reward: -15.756540500975245\n",
      "4640: Done after 40 steps. Reward: 9.285712641136803\n",
      "4660: Done after 94 steps. Reward: -13.267997605633242\n",
      "4680: Done after 50 steps. Reward: -13.779001359842313\n",
      "4700: Done after 31 steps. Reward: 9.526569532102567\n",
      "4720: Done after 120 steps. Reward: -17.091543118280338\n",
      "4740: Done after 109 steps. Reward: 7.5468743567599645\n",
      "4760: Done after 163 steps. Reward: -20.616395333975234\n",
      "4780: Done after 251 steps. Reward: -6.975122171739985\n",
      "4800: Done after 157 steps. Reward: -12.630374118381686\n",
      "Evaluation after 4800 episodes: -18.21544852530379 took 35.93137240409851s\n",
      "4820: Done after 251 steps. Reward: -9.24331827746194\n",
      "4840: Done after 52 steps. Reward: 8.846268604888937\n",
      "4860: Done after 251 steps. Reward: -12.472067853211318\n",
      "4880: Done after 222 steps. Reward: -21.427101835560233\n",
      "4900: Done after 75 steps. Reward: 8.927403358624822\n",
      "4920: Done after 131 steps. Reward: -13.948624319693158\n",
      "4940: Done after 72 steps. Reward: 9.073265519473166\n",
      "4960: Done after 251 steps. Reward: -4.146692395330634\n",
      "4980: Done after 55 steps. Reward: -12.915860737989705\n",
      "5000: Done after 85 steps. Reward: 9.184264967637379\n",
      "Evaluation after 5000 episodes: -16.112946902234594 took 35.98470211029053s\n",
      "5020: Done after 69 steps. Reward: 9.566762629113787\n",
      "5040: Done after 44 steps. Reward: 9.553166120586361\n",
      "5060: Done after 47 steps. Reward: 9.45040916414271\n",
      "5080: Done after 251 steps. Reward: -6.472848849075634\n",
      "5100: Done after 37 steps. Reward: 9.63858523891833\n",
      "5120: Done after 51 steps. Reward: -13.023237670390959\n",
      "5140: Done after 43 steps. Reward: 9.122129644433493\n",
      "5160: Done after 126 steps. Reward: 8.765861125781113\n",
      "5180: Done after 50 steps. Reward: -12.801448423193767\n",
      "5200: Done after 251 steps. Reward: -6.591981727252826\n",
      "Evaluation after 5200 episodes: -15.695518617403286 took 32.721128702163696s\n",
      "5220: Done after 219 steps. Reward: -15.07903179584012\n",
      "5240: Done after 101 steps. Reward: -12.553474015860946\n",
      "5260: Done after 78 steps. Reward: 8.910889922104792\n",
      "5280: Done after 39 steps. Reward: 9.106610866798132\n",
      "5300: Done after 29 steps. Reward: 9.63299307194387\n",
      "5320: Done after 52 steps. Reward: -13.279209464867353\n",
      "5340: Done after 71 steps. Reward: -13.867049237305604\n",
      "5360: Done after 40 steps. Reward: 9.13575097717969\n",
      "5380: Done after 72 steps. Reward: -12.728457237244864\n",
      "5400: Done after 251 steps. Reward: -4.29070028286173\n",
      "Evaluation after 5400 episodes: -14.799858946710753 took 37.02880096435547s\n",
      "5420: Done after 37 steps. Reward: 9.078462472314834\n",
      "5440: Done after 75 steps. Reward: 8.876631908673438\n",
      "5460: Done after 251 steps. Reward: -0.5842579338881593\n",
      "5480: Done after 18 steps. Reward: -10.918689831649013\n",
      "5500: Done after 53 steps. Reward: -12.002834161213576\n",
      "5520: Done after 51 steps. Reward: -13.373273844831957\n",
      "5540: Done after 148 steps. Reward: 8.964076012969397\n",
      "5560: Done after 251 steps. Reward: -5.080611429005309\n",
      "5580: Done after 106 steps. Reward: -16.977976688986203\n",
      "5600: Done after 105 steps. Reward: 9.003426718503643\n",
      "Evaluation after 5600 episodes: -14.95586973288129 took 32.94917631149292s\n",
      "5620: Done after 108 steps. Reward: 7.017426761622059\n",
      "5640: Done after 41 steps. Reward: 8.91093340275071\n",
      "5660: Done after 127 steps. Reward: -15.260467987741276\n",
      "5680: Done after 39 steps. Reward: 9.402305047363477\n",
      "5700: Done after 177 steps. Reward: -17.246682651610758\n",
      "5720: Done after 216 steps. Reward: -17.12219297834258\n",
      "5740: Done after 251 steps. Reward: -4.114767433476582\n",
      "5760: Done after 75 steps. Reward: -12.09678183834677\n",
      "5780: Done after 35 steps. Reward: 9.478909260878575\n",
      "5800: Done after 108 steps. Reward: -15.441542155713403\n",
      "Evaluation after 5800 episodes: -14.962589396325985 took 35.583492040634155s\n",
      "5820: Done after 46 steps. Reward: 9.044957293116035\n",
      "5840: Done after 175 steps. Reward: -12.819065534353445\n",
      "5860: Done after 36 steps. Reward: 9.703638859260582\n",
      "5880: Done after 89 steps. Reward: -14.566015693912183\n",
      "5900: Done after 78 steps. Reward: -13.252458122879926\n",
      "5920: Done after 55 steps. Reward: -12.088302168292852\n",
      "5940: Done after 139 steps. Reward: 8.239952210192099\n",
      "5960: Done after 147 steps. Reward: 7.30868885146092\n",
      "5980: Done after 70 steps. Reward: -13.381434448019142\n",
      "6000: Done after 87 steps. Reward: -13.01754562203661\n",
      "Evaluation after 6000 episodes: -16.55949715965776 took 35.346266984939575s\n",
      "6020: Done after 89 steps. Reward: -15.771642531144025\n",
      "6040: Done after 55 steps. Reward: -12.513495435700957\n",
      "6060: Done after 215 steps. Reward: -16.41795695497949\n",
      "6080: Done after 110 steps. Reward: -12.622688644879423\n",
      "6100: Done after 193 steps. Reward: 5.066496459611371\n",
      "6120: Done after 88 steps. Reward: 8.980970750795878\n",
      "6140: Done after 65 steps. Reward: -12.956530804835344\n",
      "6160: Done after 65 steps. Reward: -13.522026757198507\n",
      "6180: Done after 30 steps. Reward: 9.632703115341739\n",
      "6200: Done after 142 steps. Reward: -15.53834214059291\n",
      "Evaluation after 6200 episodes: -15.12465719004611 took 32.219902992248535s\n",
      "6220: Done after 76 steps. Reward: -13.073420125076327\n",
      "6240: Done after 92 steps. Reward: -18.21461645248067\n",
      "6260: Done after 103 steps. Reward: -20.136398309893135\n",
      "6280: Done after 251 steps. Reward: -11.338848598662851\n",
      "6300: Done after 37 steps. Reward: 9.42093963989529\n",
      "6320: Done after 36 steps. Reward: 9.678360003836278\n",
      "6340: Done after 171 steps. Reward: 3.0983855503884694\n",
      "6360: Done after 140 steps. Reward: -13.645609243520767\n",
      "6380: Done after 125 steps. Reward: -18.208596726047286\n",
      "6400: Done after 69 steps. Reward: -12.6551315222364\n",
      "Evaluation after 6400 episodes: -12.801639072700173 took 32.06448698043823s\n",
      "6420: Done after 103 steps. Reward: -14.524694263222703\n",
      "6440: Done after 40 steps. Reward: 9.164090138553856\n",
      "6460: Done after 251 steps. Reward: -5.98083615068953\n",
      "6480: Done after 42 steps. Reward: 9.317190175583235\n",
      "6500: Done after 143 steps. Reward: -14.439024944566587\n",
      "6520: Done after 163 steps. Reward: -18.598133483029663\n",
      "6540: Done after 36 steps. Reward: 9.67983772585258\n",
      "6560: Done after 38 steps. Reward: 9.523214331240395\n",
      "6580: Done after 113 steps. Reward: 8.768638243456074\n",
      "6600: Done after 91 steps. Reward: 8.908518984033472\n",
      "Evaluation after 6600 episodes: -12.954606583727323 took 32.115697145462036s\n",
      "6620: Done after 42 steps. Reward: 9.166917588092808\n",
      "6640: Done after 131 steps. Reward: -14.675310297100484\n",
      "6660: Done after 166 steps. Reward: -14.012884861756758\n",
      "6680: Done after 56 steps. Reward: -12.40652519182025\n",
      "6700: Done after 43 steps. Reward: 9.55985984093476\n",
      "6720: Done after 251 steps. Reward: -7.223494491584003\n",
      "6740: Done after 39 steps. Reward: 9.185392964279728\n",
      "6760: Done after 32 steps. Reward: 9.750050047107456\n",
      "6780: Done after 251 steps. Reward: -3.474948976840165\n",
      "6800: Done after 53 steps. Reward: -12.22415596901532\n",
      "Evaluation after 6800 episodes: -16.051710842208998 took 33.75918626785278s\n",
      "6820: Done after 203 steps. Reward: 7.474517700452963\n",
      "6840: Done after 108 steps. Reward: 7.8657148519092885\n",
      "6860: Done after 251 steps. Reward: -8.770039470405528\n",
      "6880: Done after 138 steps. Reward: 1.7566946753663473\n",
      "6900: Done after 251 steps. Reward: -6.422705142863512\n",
      "6920: Done after 52 steps. Reward: 9.362752982206985\n",
      "6940: Done after 111 steps. Reward: -15.049086458532193\n",
      "6960: Done after 162 steps. Reward: -14.633392089844216\n",
      "6980: Done after 97 steps. Reward: -13.17124015028334\n",
      "7000: Done after 251 steps. Reward: -8.815565064135269\n",
      "Evaluation after 7000 episodes: -14.379038191854557 took 32.08663535118103s\n",
      "7020: Done after 58 steps. Reward: -12.921373355091204\n",
      "7040: Done after 100 steps. Reward: 8.349339884235254\n",
      "7060: Done after 51 steps. Reward: -11.989141687620787\n",
      "7080: Done after 36 steps. Reward: 9.515594049781637\n",
      "7100: Done after 230 steps. Reward: 0.5111034486676438\n",
      "7120: Done after 251 steps. Reward: -0.4598295392873027\n",
      "7140: Done after 147 steps. Reward: 8.779357161218844\n",
      "7160: Done after 72 steps. Reward: 9.496950836147157\n",
      "7180: Done after 60 steps. Reward: 9.346852563093362\n",
      "7200: Done after 78 steps. Reward: -12.689449425773919\n",
      "Evaluation after 7200 episodes: -14.171703400441604 took 34.15433311462402s\n",
      "7220: Done after 42 steps. Reward: 9.468222923715203\n",
      "7240: Done after 49 steps. Reward: -12.812124690028842\n",
      "7260: Done after 37 steps. Reward: 9.37066548804042\n",
      "7280: Done after 251 steps. Reward: -7.982172807777404\n",
      "7300: Done after 251 steps. Reward: -0.641261050446322\n",
      "7320: Done after 59 steps. Reward: 9.151721945486614\n",
      "7340: Done after 103 steps. Reward: 8.559875284256218\n",
      "7360: Done after 40 steps. Reward: 9.42970333936322\n",
      "7380: Done after 30 steps. Reward: 9.561843647254877\n",
      "7400: Done after 251 steps. Reward: -4.770970370831495\n",
      "Evaluation after 7400 episodes: -15.427810195838571 took 31.14071035385132s\n",
      "7420: Done after 42 steps. Reward: 9.294454233166547\n",
      "7440: Done after 51 steps. Reward: -14.262119830773457\n",
      "7460: Done after 147 steps. Reward: -17.949166631696492\n",
      "7480: Done after 251 steps. Reward: -0.9900650862984152\n",
      "7500: Done after 106 steps. Reward: -12.78432562214196\n",
      "7520: Done after 36 steps. Reward: 9.599437219538654\n",
      "7540: Done after 54 steps. Reward: -13.256733259546108\n",
      "7560: Done after 84 steps. Reward: 9.529274860811663\n",
      "7580: Done after 42 steps. Reward: 9.073279720564297\n",
      "7600: Done after 44 steps. Reward: -11.742562961374713\n",
      "Evaluation after 7600 episodes: -17.447507924076533 took 33.12909173965454s\n",
      "New best model with 11.54% improvement... Saving model as checkpoint\n",
      "Saved model to models/checkpoints/Hockey_Rainbow_run7_cp2.pt\n",
      "7620: Done after 48 steps. Reward: -11.67795394389245\n",
      "7640: Done after 34 steps. Reward: 9.55023796206681\n",
      "7660: Done after 63 steps. Reward: -12.971811154364268\n",
      "7680: Done after 98 steps. Reward: -12.165061100405653\n",
      "7700: Done after 34 steps. Reward: 9.163295237583323\n",
      "7720: Done after 37 steps. Reward: 9.281321590188709\n",
      "7740: Done after 43 steps. Reward: 8.879946265688648\n",
      "7760: Done after 33 steps. Reward: 9.202353959358996\n",
      "7780: Done after 64 steps. Reward: -12.929496038395657\n",
      "7800: Done after 110 steps. Reward: 6.302085087534032\n",
      "Evaluation after 7800 episodes: -19.196772693022027 took 32.53879904747009s\n",
      "7820: Done after 53 steps. Reward: -14.078294582320728\n",
      "7840: Done after 97 steps. Reward: 8.712079717401066\n",
      "7860: Done after 47 steps. Reward: 9.102383192957884\n",
      "7880: Done after 251 steps. Reward: -4.19700123722864\n",
      "7900: Done after 51 steps. Reward: -14.233997433612021\n",
      "7920: Done after 127 steps. Reward: 7.672632640922857\n",
      "7940: Done after 67 steps. Reward: 9.24241001562174\n",
      "7960: Done after 39 steps. Reward: 9.47236589286978\n",
      "7980: Done after 251 steps. Reward: -0.8721119573939692\n",
      "8000: Done after 58 steps. Reward: 9.255130626503664\n",
      "Evaluation after 8000 episodes: -16.132884702053776 took 34.18619394302368s\n",
      "8020: Done after 99 steps. Reward: 8.302621720955951\n",
      "8040: Done after 63 steps. Reward: -14.489959827881602\n",
      "8060: Done after 251 steps. Reward: -15.297776539927277\n",
      "8080: Done after 75 steps. Reward: -17.369380329430072\n",
      "8100: Done after 38 steps. Reward: 9.434722002843147\n",
      "8120: Done after 70 steps. Reward: -14.062190669238634\n",
      "8140: Done after 36 steps. Reward: 9.691512922785954\n",
      "8160: Done after 31 steps. Reward: 9.509824243770412\n",
      "8180: Done after 107 steps. Reward: -17.244308384805066\n",
      "8200: Done after 37 steps. Reward: 9.543125522763754\n",
      "Evaluation after 8200 episodes: -19.872083271136656 took 33.43784785270691s\n",
      "8220: Done after 36 steps. Reward: 9.397724092344916\n",
      "8240: Done after 251 steps. Reward: -12.859012018934537\n",
      "8260: Done after 251 steps. Reward: -31.490458969991796\n",
      "8280: Done after 223 steps. Reward: -17.617463860898113\n",
      "8300: Done after 41 steps. Reward: 9.49819585221428\n",
      "8320: Done after 81 steps. Reward: -14.375245542694799\n",
      "8340: Done after 43 steps. Reward: 9.15116938010045\n",
      "8360: Done after 162 steps. Reward: 6.444759363331705\n",
      "8380: Done after 102 steps. Reward: 8.551884427611627\n",
      "8400: Done after 57 steps. Reward: -12.324394877746144\n",
      "Evaluation after 8400 episodes: -21.25257305519774 took 34.08082103729248s\n",
      "8420: Done after 193 steps. Reward: -18.473392390180635\n",
      "8440: Done after 80 steps. Reward: -13.498390586299314\n",
      "8460: Done after 59 steps. Reward: -11.857445979872898\n",
      "8480: Done after 113 steps. Reward: -13.212809448580124\n",
      "8500: Done after 129 steps. Reward: 7.776027680596849\n",
      "8520: Done after 52 steps. Reward: -12.597091956086032\n",
      "8540: Done after 133 steps. Reward: -16.56043218443841\n",
      "8560: Done after 60 steps. Reward: 9.09341759519318\n",
      "8580: Done after 137 steps. Reward: -18.042928999215185\n",
      "8600: Done after 251 steps. Reward: -23.772574797896752\n",
      "Evaluation after 8600 episodes: -21.170757630039684 took 33.35661220550537s\n",
      "8620: Done after 148 steps. Reward: 5.563117673096038\n",
      "8640: Done after 40 steps. Reward: 9.072473018354048\n",
      "8660: Done after 49 steps. Reward: -12.437441386065245\n",
      "8680: Done after 251 steps. Reward: -19.74047189631967\n",
      "8700: Done after 251 steps. Reward: -10.76788133374805\n",
      "8720: Done after 38 steps. Reward: 9.634757340033547\n",
      "8740: Done after 251 steps. Reward: -17.354012622918173\n",
      "8760: Done after 169 steps. Reward: -16.046556797068718\n",
      "8780: Done after 44 steps. Reward: 9.637631598676245\n",
      "8800: Done after 105 steps. Reward: -11.973732357978532\n",
      "Evaluation after 8800 episodes: -19.9750620367736 took 32.96121001243591s\n",
      "8820: Done after 132 steps. Reward: 8.579470682948887\n",
      "8840: Done after 51 steps. Reward: -12.144390728520793\n",
      "8860: Done after 95 steps. Reward: -17.61284044140244\n",
      "8880: Done after 84 steps. Reward: 9.318344265160071\n",
      "8900: Done after 183 steps. Reward: -19.290159264496353\n",
      "8920: Done after 173 steps. Reward: -18.346401258615952\n",
      "8940: Done after 33 steps. Reward: 9.264893442119561\n",
      "8960: Done after 251 steps. Reward: -0.8136406935387358\n",
      "8980: Done after 69 steps. Reward: 8.379505035447393\n",
      "9000: Done after 105 steps. Reward: 9.021406883072705\n",
      "Evaluation after 9000 episodes: -19.811710469837884 took 33.87456750869751s\n",
      "9020: Done after 43 steps. Reward: 9.077836060012697\n",
      "9040: Done after 207 steps. Reward: -0.20566918830190772\n",
      "9060: Done after 36 steps. Reward: 9.575001032931862\n",
      "9080: Done after 152 steps. Reward: -15.473491832820981\n",
      "9100: Done after 53 steps. Reward: -13.349105846076997\n",
      "9120: Done after 130 steps. Reward: 6.257331923108313\n",
      "9140: Done after 83 steps. Reward: 8.342209532884752\n",
      "9160: Done after 251 steps. Reward: -22.15792900883819\n",
      "9180: Done after 33 steps. Reward: 9.37721908598547\n",
      "9200: Done after 100 steps. Reward: 8.906208117382803\n",
      "Evaluation after 9200 episodes: -18.64914380746312 took 36.30630612373352s\n",
      "9220: Done after 41 steps. Reward: 9.37899193086641\n",
      "9240: Done after 67 steps. Reward: -13.359425977231124\n",
      "9260: Done after 126 steps. Reward: 6.004355844477155\n",
      "9280: Done after 65 steps. Reward: 9.135896820392519\n",
      "9300: Done after 81 steps. Reward: 9.248767319168728\n",
      "9320: Done after 39 steps. Reward: 9.37691325975937\n",
      "9340: Done after 37 steps. Reward: 9.561066986651088\n",
      "9360: Done after 171 steps. Reward: 5.026133658705052\n",
      "9380: Done after 31 steps. Reward: 9.473157829332845\n",
      "9400: Done after 62 steps. Reward: 9.144623781101533\n",
      "Evaluation after 9400 episodes: -20.8795894435577 took 36.45769548416138s\n",
      "9420: Done after 251 steps. Reward: -14.399963394444114\n",
      "9440: Done after 47 steps. Reward: 9.355075233887108\n",
      "9460: Done after 40 steps. Reward: 9.302330323797289\n",
      "9480: Done after 32 steps. Reward: 9.35029201601667\n",
      "9500: Done after 36 steps. Reward: 9.471044170860925\n",
      "9520: Done after 251 steps. Reward: -7.748471734562225\n",
      "9540: Done after 41 steps. Reward: 9.052662303495806\n",
      "9560: Done after 91 steps. Reward: 9.066829510193825\n",
      "9580: Done after 251 steps. Reward: -7.023220866128811\n",
      "9600: Done after 47 steps. Reward: 9.187117161260048\n",
      "Evaluation after 9600 episodes: -16.758303144814782 took 34.43686890602112s\n",
      "9620: Done after 81 steps. Reward: 8.754524211729157\n",
      "9640: Done after 90 steps. Reward: 9.133598622857415\n",
      "9660: Done after 105 steps. Reward: -14.615290507502058\n",
      "9680: Done after 93 steps. Reward: -12.836818303916026\n",
      "9700: Done after 62 steps. Reward: 8.870036443141824\n",
      "9720: Done after 43 steps. Reward: -11.535551627426731\n",
      "9740: Done after 127 steps. Reward: 5.975725332896436\n",
      "9760: Done after 90 steps. Reward: -11.91815174464281\n",
      "9780: Done after 251 steps. Reward: -3.942374160145952\n",
      "9800: Done after 90 steps. Reward: 9.370252120422418\n",
      "Evaluation after 9800 episodes: -17.94297428562615 took 36.11705684661865s\n",
      "9820: Done after 35 steps. Reward: 9.366140187429485\n",
      "9840: Done after 39 steps. Reward: 9.359952278051878\n",
      "9860: Done after 107 steps. Reward: -12.793613185549205\n",
      "9880: Done after 41 steps. Reward: 9.10070754756241\n",
      "9900: Done after 48 steps. Reward: -11.990111154051982\n",
      "9920: Done after 34 steps. Reward: 9.221649383006755\n",
      "9940: Done after 45 steps. Reward: -11.71990028631495\n",
      "9960: Done after 251 steps. Reward: -3.9835012475077174\n",
      "9980: Done after 39 steps. Reward: 9.345820997284214\n",
      "10000: Done after 109 steps. Reward: -15.088125274525959\n",
      "Evaluation after 10000 episodes: -20.993073766890433 took 33.64385008811951s\n",
      "10020: Done after 37 steps. Reward: 9.417786001443638\n",
      "10040: Done after 104 steps. Reward: 7.664128082693786\n",
      "10060: Done after 80 steps. Reward: 8.870369133443313\n",
      "10080: Done after 163 steps. Reward: -21.890605621576746\n",
      "10100: Done after 69 steps. Reward: -15.115422971400928\n",
      "10120: Done after 251 steps. Reward: -6.664284269008763\n",
      "10140: Done after 41 steps. Reward: 9.273218587434835\n",
      "10160: Done after 251 steps. Reward: -9.190689972094821\n",
      "10180: Done after 213 steps. Reward: -0.37890199526334456\n",
      "10200: Done after 170 steps. Reward: -17.893648202375953\n",
      "Evaluation after 10200 episodes: -19.800261147608982 took 34.891409397125244s\n",
      "10220: Done after 251 steps. Reward: -7.731381488678474\n",
      "10240: Done after 38 steps. Reward: 9.399758865666378\n",
      "10260: Done after 75 steps. Reward: 9.299931845493221\n",
      "10280: Done after 47 steps. Reward: -12.017127734995062\n",
      "10300: Done after 55 steps. Reward: -14.368237825232507\n",
      "10320: Done after 79 steps. Reward: 7.836671057771187\n",
      "10340: Done after 50 steps. Reward: -13.608709432051764\n",
      "10360: Done after 69 steps. Reward: -12.901052259334161\n",
      "10380: Done after 97 steps. Reward: 9.317669804145362\n",
      "10400: Done after 165 steps. Reward: 6.935723077798294\n",
      "Evaluation after 10400 episodes: -18.8977316325583 took 39.92133283615112s\n",
      "10420: Done after 56 steps. Reward: -13.168975191254892\n",
      "10440: Done after 251 steps. Reward: -7.9531217004450525\n",
      "10460: Done after 41 steps. Reward: 9.271323425788516\n",
      "10480: Done after 214 steps. Reward: -17.924089147309797\n",
      "10500: Done after 59 steps. Reward: 8.147712411031183\n",
      "10520: Done after 69 steps. Reward: 9.349874351285882\n",
      "10540: Done after 98 steps. Reward: -13.575875796343777\n",
      "10560: Done after 87 steps. Reward: -15.643213172767641\n",
      "10580: Done after 59 steps. Reward: 9.166028616436941\n",
      "10600: Done after 75 steps. Reward: -14.85361053870329\n",
      "Evaluation after 10600 episodes: -20.969796339647438 took 37.78661823272705s\n",
      "10620: Done after 251 steps. Reward: -6.276956580656225\n",
      "10640: Done after 251 steps. Reward: -14.421712674985498\n",
      "10660: Done after 56 steps. Reward: -12.520376659555541\n",
      "10680: Done after 31 steps. Reward: 9.367555265652596\n",
      "10700: Done after 34 steps. Reward: 9.254086297684573\n",
      "10720: Done after 98 steps. Reward: 8.399382367458843\n",
      "10740: Done after 160 steps. Reward: -13.65828649116483\n",
      "10760: Done after 109 steps. Reward: 7.1269177214517265\n",
      "10780: Done after 43 steps. Reward: 9.222381565586593\n",
      "10800: Done after 251 steps. Reward: -10.327632800206034\n",
      "Evaluation after 10800 episodes: -21.722779700324587 took 34.201855421066284s\n",
      "10820: Done after 41 steps. Reward: 9.372638938039358\n",
      "10840: Done after 251 steps. Reward: -23.792649791133467\n",
      "10860: Done after 46 steps. Reward: -11.734397995262114\n",
      "10880: Done after 251 steps. Reward: -1.2918625862594773\n",
      "10900: Done after 42 steps. Reward: 9.419568655545502\n",
      "10920: Done after 66 steps. Reward: -13.011395567129728\n",
      "10940: Done after 251 steps. Reward: -15.36889794076791\n",
      "10960: Done after 32 steps. Reward: 9.757613497379051\n",
      "10980: Done after 36 steps. Reward: 9.707312571148927\n",
      "11000: Done after 41 steps. Reward: -11.877874313521112\n",
      "Evaluation after 11000 episodes: -20.028021622815995 took 36.58666777610779s\n",
      "11020: Done after 251 steps. Reward: -3.1519624320264303\n",
      "11040: Done after 63 steps. Reward: 8.728369337197957\n",
      "11060: Done after 149 steps. Reward: 5.340175436980946\n",
      "11080: Done after 35 steps. Reward: -12.904479226045392\n",
      "11100: Done after 61 steps. Reward: 9.331082757374228\n",
      "11120: Done after 39 steps. Reward: 9.350703014490785\n",
      "11140: Done after 157 steps. Reward: 2.7956743471328513\n",
      "11160: Done after 31 steps. Reward: 9.603230662869377\n",
      "11180: Done after 32 steps. Reward: 9.45088204362599\n",
      "11200: Done after 33 steps. Reward: 9.190800559323586\n",
      "Evaluation after 11200 episodes: -19.203368241228628 took 37.556227922439575s\n",
      "11220: Done after 65 steps. Reward: -12.631252544872051\n",
      "11240: Done after 36 steps. Reward: 9.346732160959167\n",
      "11260: Done after 67 steps. Reward: 9.450985173746325\n",
      "11280: Done after 71 steps. Reward: 9.087673284926364\n",
      "11300: Done after 74 steps. Reward: 9.515831438269759\n",
      "11320: Done after 72 steps. Reward: 9.093837601501654\n",
      "11340: Done after 62 steps. Reward: -14.041720601286084\n",
      "11360: Done after 82 steps. Reward: -15.358634002465458\n",
      "11380: Done after 59 steps. Reward: -14.036174225091075\n",
      "11400: Done after 35 steps. Reward: 9.003219921779953\n",
      "Evaluation after 11400 episodes: -17.976419367944754 took 35.36455154418945s\n",
      "11420: Done after 58 steps. Reward: 9.244409812037954\n",
      "11440: Done after 73 steps. Reward: -13.449510725820703\n",
      "11460: Done after 42 steps. Reward: 9.157509947990649\n",
      "11480: Done after 27 steps. Reward: 9.656171844988862\n",
      "11500: Done after 87 steps. Reward: 7.84656005546705\n",
      "11520: Done after 41 steps. Reward: 9.183834175721682\n",
      "11540: Done after 147 steps. Reward: -12.58109655430652\n",
      "11560: Done after 29 steps. Reward: 9.601545587475353\n",
      "11580: Done after 39 steps. Reward: 9.68088825681502\n",
      "11600: Done after 34 steps. Reward: 9.135615699262273\n",
      "Evaluation after 11600 episodes: -19.515963122439217 took 37.447325706481934s\n",
      "11620: Done after 84 steps. Reward: -13.300580388502304\n",
      "11640: Done after 111 steps. Reward: -12.330346174570469\n",
      "11660: Done after 139 steps. Reward: 6.385595617422231\n",
      "11680: Done after 93 steps. Reward: -13.810527814696748\n",
      "11700: Done after 42 steps. Reward: 9.042170099441432\n",
      "11720: Done after 43 steps. Reward: 9.150354889187874\n",
      "11740: Done after 63 steps. Reward: 8.970756428471704\n",
      "11760: Done after 40 steps. Reward: 9.487381750136258\n",
      "11780: Done after 251 steps. Reward: -7.177540804689167\n",
      "11800: Done after 60 steps. Reward: -12.756007530172328\n",
      "Evaluation after 11800 episodes: -20.154859804507133 took 36.39555287361145s\n",
      "11820: Done after 30 steps. Reward: 9.628448473016702\n",
      "11840: Done after 124 steps. Reward: -13.721917169649283\n",
      "11860: Done after 152 steps. Reward: -18.79710732964412\n",
      "11880: Done after 35 steps. Reward: 9.591958915968915\n",
      "11900: Done after 48 steps. Reward: -12.287766322771187\n",
      "11920: Done after 31 steps. Reward: 9.486194561793962\n",
      "11940: Done after 251 steps. Reward: -25.76163041202791\n",
      "11960: Done after 109 steps. Reward: 6.445344081187664\n",
      "11980: Done after 73 steps. Reward: 8.755805097989718\n",
      "12000: Done after 91 steps. Reward: 9.392224522207458\n",
      "Evaluation after 12000 episodes: -21.36852727744602 took 39.09067368507385s\n",
      "12020: Done after 70 steps. Reward: -13.875873614566485\n",
      "12040: Done after 52 steps. Reward: -12.920681572440147\n",
      "12060: Done after 60 steps. Reward: 9.292435454994033\n",
      "12080: Done after 68 steps. Reward: -13.384365626902488\n",
      "12100: Done after 75 steps. Reward: 9.123328203821218\n",
      "12120: Done after 43 steps. Reward: 8.907821442321994\n",
      "12140: Done after 30 steps. Reward: 9.420340185749174\n",
      "12160: Done after 38 steps. Reward: 9.46897103534086\n",
      "12180: Done after 251 steps. Reward: -5.368390586221353\n",
      "12200: Done after 105 steps. Reward: 8.197712398385447\n",
      "Evaluation after 12200 episodes: -20.422523943917184 took 37.49502897262573s\n",
      "12220: Done after 251 steps. Reward: -24.20852106290475\n",
      "12240: Done after 48 steps. Reward: -12.43402298852825\n",
      "12260: Done after 57 steps. Reward: -12.874217363223014\n",
      "12280: Done after 69 steps. Reward: 9.253910021267545\n",
      "12300: Done after 46 steps. Reward: 9.323625091816139\n",
      "12320: Done after 251 steps. Reward: -5.981391539879495\n",
      "12340: Done after 48 steps. Reward: -11.834498382958357\n",
      "12360: Done after 67 steps. Reward: 9.088049534120625\n",
      "12380: Done after 35 steps. Reward: 9.039957089670365\n",
      "12400: Done after 39 steps. Reward: 9.173955527988097\n",
      "Evaluation after 12400 episodes: -21.043178700541517 took 35.78832960128784s\n",
      "12420: Done after 61 steps. Reward: 9.074734876705485\n",
      "12440: Done after 84 steps. Reward: 9.518632690107001\n",
      "12460: Done after 121 steps. Reward: -13.886598771773397\n",
      "12480: Done after 163 steps. Reward: -13.259535043204885\n",
      "12500: Done after 251 steps. Reward: -0.5652752289900993\n",
      "12520: Done after 251 steps. Reward: -20.51619576018758\n",
      "12540: Done after 37 steps. Reward: 9.384983163605561\n",
      "12560: Done after 36 steps. Reward: 9.160948342807371\n",
      "12580: Done after 136 steps. Reward: -14.67742158701223\n",
      "12600: Done after 59 steps. Reward: -14.237583117779916\n",
      "Evaluation after 12600 episodes: -18.602047611361648 took 34.958402156829834s\n",
      "12620: Done after 124 steps. Reward: -15.999585918171263\n",
      "12640: Done after 65 steps. Reward: 8.926358403825844\n",
      "12660: Done after 60 steps. Reward: -11.923764843328483\n",
      "12680: Done after 36 steps. Reward: 9.668303912694295\n",
      "12700: Done after 33 steps. Reward: 9.295780783240476\n",
      "12720: Done after 52 steps. Reward: -13.245132865559787\n",
      "12740: Done after 47 steps. Reward: -11.78693713922929\n",
      "12760: Done after 38 steps. Reward: 9.598862157717168\n",
      "12780: Done after 136 steps. Reward: 6.686772715118315\n",
      "12800: Done after 54 steps. Reward: -12.567988038237313\n",
      "Evaluation after 12800 episodes: -21.06779724348352 took 36.39008283615112s\n",
      "12820: Done after 251 steps. Reward: -8.90380305204995\n",
      "12840: Done after 52 steps. Reward: -12.02413199630282\n",
      "12860: Done after 204 steps. Reward: -18.69292209268614\n",
      "12880: Done after 65 steps. Reward: -14.170486752364747\n",
      "12900: Done after 35 steps. Reward: 8.952808357531177\n",
      "12920: Done after 30 steps. Reward: 9.566248009519757\n",
      "12940: Done after 67 steps. Reward: 8.61168302174356\n",
      "12960: Done after 88 steps. Reward: 9.092698290032137\n",
      "12980: Done after 176 steps. Reward: 2.074216001390252\n",
      "13000: Done after 60 steps. Reward: 9.417044722658536\n",
      "Evaluation after 13000 episodes: -21.203869524193447 took 36.98539447784424s\n",
      "13020: Done after 106 steps. Reward: 8.143740599170544\n",
      "13040: Done after 60 steps. Reward: 9.522873825681966\n",
      "13060: Done after 46 steps. Reward: -11.836014939288242\n",
      "13080: Done after 46 steps. Reward: 9.185639460617503\n",
      "13100: Done after 61 steps. Reward: 9.278479829800368\n",
      "13120: Done after 33 steps. Reward: 9.319784337373935\n",
      "13140: Done after 59 steps. Reward: 9.358516372234115\n",
      "13160: Done after 39 steps. Reward: 9.525187266029052\n",
      "13180: Done after 60 steps. Reward: 9.287551600667689\n",
      "13200: Done after 37 steps. Reward: 9.3275355564225\n",
      "Evaluation after 13200 episodes: -17.291693568349295 took 37.878973960876465s\n",
      "13220: Done after 251 steps. Reward: -19.402922508771457\n",
      "13240: Done after 86 steps. Reward: 9.235189792136984\n",
      "13260: Done after 43 steps. Reward: 9.347513765498926\n",
      "13280: Done after 170 steps. Reward: 7.773774108948327\n",
      "13300: Done after 68 steps. Reward: 9.628328854252041\n",
      "13320: Done after 66 steps. Reward: 9.13374525415666\n",
      "13340: Done after 32 steps. Reward: 9.389910701008128\n",
      "13360: Done after 49 steps. Reward: -12.439408041289136\n",
      "13380: Done after 52 steps. Reward: -12.164083025481109\n",
      "13400: Done after 58 steps. Reward: 9.23203173746492\n",
      "Evaluation after 13400 episodes: -17.550422474881554 took 33.17325830459595s\n",
      "13420: Done after 251 steps. Reward: -11.835808732655241\n",
      "13440: Done after 30 steps. Reward: 9.424185976742319\n",
      "13460: Done after 154 steps. Reward: 3.745298633714871\n",
      "13480: Done after 251 steps. Reward: -25.903348348954747\n",
      "13500: Done after 54 steps. Reward: -12.742025773358494\n",
      "13520: Done after 84 steps. Reward: -15.15321886615509\n",
      "13540: Done after 251 steps. Reward: -11.928619361040948\n",
      "13560: Done after 30 steps. Reward: 9.505131742336587\n",
      "13580: Done after 168 steps. Reward: -18.253603359856996\n",
      "13600: Done after 60 steps. Reward: 9.332708206808004\n",
      "Evaluation after 13600 episodes: -18.848360914171195 took 35.2356333732605s\n",
      "13620: Done after 30 steps. Reward: 9.49614209912949\n",
      "13640: Done after 34 steps. Reward: 8.979081603190872\n",
      "13660: Done after 34 steps. Reward: 9.012989169097384\n",
      "13680: Done after 251 steps. Reward: -4.5669659519468215\n",
      "13700: Done after 85 steps. Reward: 8.867857374051233\n",
      "13720: Done after 33 steps. Reward: 9.661960412549691\n",
      "13740: Done after 33 steps. Reward: 9.310580503520132\n",
      "13760: Done after 76 steps. Reward: 9.19546356083795\n",
      "13780: Done after 32 steps. Reward: 9.323624603177667\n",
      "13800: Done after 152 steps. Reward: 5.059541324370149\n",
      "Evaluation after 13800 episodes: -24.298453402004498 took 36.74766826629639s\n",
      "13820: Done after 102 steps. Reward: 8.610953121802396\n",
      "13840: Done after 101 steps. Reward: 8.266869940576193\n",
      "13860: Done after 34 steps. Reward: 9.222183198047745\n",
      "13880: Done after 80 steps. Reward: -12.367169598869069\n",
      "13900: Done after 61 steps. Reward: -11.867623370931318\n",
      "13920: Done after 251 steps. Reward: -17.481721352871244\n",
      "13940: Done after 28 steps. Reward: 9.519690063941736\n",
      "13960: Done after 52 steps. Reward: -12.664877592056442\n",
      "13980: Done after 69 steps. Reward: -11.762036358099174\n",
      "14000: Done after 37 steps. Reward: 9.634269006297504\n",
      "Evaluation after 14000 episodes: -20.127231559750427 took 36.4576358795166s\n",
      "14020: Done after 70 steps. Reward: 9.277919651662716\n",
      "14040: Done after 37 steps. Reward: 9.056432731803735\n",
      "14060: Done after 251 steps. Reward: -9.496121080744874\n",
      "14080: Done after 30 steps. Reward: 9.592894717410278\n",
      "14100: Done after 60 steps. Reward: 9.558293354363991\n",
      "14120: Done after 59 steps. Reward: 9.320416006460366\n",
      "14140: Done after 98 steps. Reward: -14.197396311247712\n",
      "14160: Done after 53 steps. Reward: -13.09712128886899\n",
      "14180: Done after 90 steps. Reward: -12.539411616004639\n",
      "14200: Done after 71 steps. Reward: 9.43788684246756\n",
      "Evaluation after 14200 episodes: -21.456744211614367 took 34.105462074279785s\n",
      "14220: Done after 33 steps. Reward: 9.609754140016047\n",
      "14240: Done after 103 steps. Reward: 8.526967997559243\n",
      "14260: Done after 31 steps. Reward: 9.545475180645395\n",
      "14280: Done after 37 steps. Reward: 9.479173340156038\n",
      "14300: Done after 35 steps. Reward: 9.402496443529207\n",
      "14320: Done after 51 steps. Reward: -12.861106484923377\n",
      "14340: Done after 165 steps. Reward: 3.0097592267469864\n",
      "14360: Done after 57 steps. Reward: -13.054289767962231\n",
      "14380: Done after 51 steps. Reward: -11.984142091235556\n",
      "14400: Done after 46 steps. Reward: -11.981359440690277\n",
      "Evaluation after 14400 episodes: -15.528624456795738 took 33.02704215049744s\n",
      "14420: Done after 172 steps. Reward: -16.454237379753508\n",
      "14440: Done after 115 steps. Reward: -15.415462282438009\n",
      "14460: Done after 40 steps. Reward: 9.26648061906806\n",
      "14480: Done after 93 steps. Reward: 8.902793505000336\n",
      "14500: Done after 122 steps. Reward: -15.25717602558513\n",
      "14520: Done after 106 steps. Reward: 7.093948440447461\n",
      "14540: Done after 251 steps. Reward: -5.43851715805427\n",
      "14560: Done after 51 steps. Reward: -13.150905794472145\n",
      "14580: Done after 150 steps. Reward: -16.0339078294296\n",
      "14600: Done after 30 steps. Reward: 9.585727275791692\n",
      "Evaluation after 14600 episodes: -27.36206233923037 took 35.544838428497314s\n",
      "14620: Done after 60 steps. Reward: 8.15778864927542\n",
      "14640: Done after 251 steps. Reward: -17.55091161873731\n",
      "14660: Done after 61 steps. Reward: 9.11052750337291\n",
      "14680: Done after 251 steps. Reward: -0.774681863889893\n",
      "14700: Done after 49 steps. Reward: -12.66049250441319\n",
      "14720: Done after 30 steps. Reward: 9.40808808795831\n",
      "14740: Done after 189 steps. Reward: 5.1866486099359275\n",
      "14760: Done after 251 steps. Reward: -8.83546357874014\n",
      "14780: Done after 33 steps. Reward: 9.276508071356872\n",
      "14800: Done after 35 steps. Reward: 9.062372368178085\n",
      "Evaluation after 14800 episodes: -21.584535935467475 took 37.6474175453186s\n",
      "14820: Done after 125 steps. Reward: 7.0488585224844575\n",
      "14840: Done after 39 steps. Reward: 9.566547013682817\n",
      "14860: Done after 29 steps. Reward: 9.7150113988786\n",
      "14880: Done after 63 steps. Reward: 9.304309105032301\n",
      "14900: Done after 86 steps. Reward: 9.328316417988793\n",
      "14920: Done after 98 steps. Reward: -15.10638038764813\n",
      "14940: Done after 54 steps. Reward: -12.72272923415673\n",
      "14960: Done after 251 steps. Reward: -8.12859211586904\n",
      "14980: Done after 37 steps. Reward: 9.388593133950652\n",
      "15000: Done after 96 steps. Reward: 8.13451918782987\n",
      "Evaluation after 15000 episodes: -23.66165047344621 took 38.45819330215454s\n",
      "15020: Done after 45 steps. Reward: 9.06939272299616\n",
      "15040: Done after 100 steps. Reward: -16.99690995593679\n",
      "15060: Done after 32 steps. Reward: 9.378892119810791\n",
      "15080: Done after 251 steps. Reward: -3.9192478769878134\n",
      "15100: Done after 251 steps. Reward: -13.212373792236304\n",
      "15120: Done after 30 steps. Reward: 9.639107007116738\n",
      "15140: Done after 251 steps. Reward: -5.16972855449019\n",
      "15160: Done after 65 steps. Reward: 9.043486318795988\n",
      "15180: Done after 251 steps. Reward: -0.27361662392348246\n",
      "15200: Done after 32 steps. Reward: 9.415721355019611\n",
      "Evaluation after 15200 episodes: -26.12287239159111 took 35.5117621421814s\n",
      "15220: Done after 33 steps. Reward: 9.42565739326732\n",
      "15240: Done after 42 steps. Reward: -11.521432310909619\n",
      "15260: Done after 68 steps. Reward: 9.200662594300063\n",
      "15280: Done after 31 steps. Reward: 9.643997301610344\n",
      "15300: Done after 106 steps. Reward: 7.287351624373125\n",
      "15320: Done after 62 steps. Reward: 8.896289075281372\n",
      "15340: Done after 29 steps. Reward: 9.61091750421939\n",
      "15360: Done after 40 steps. Reward: 9.067400292716096\n",
      "15380: Done after 251 steps. Reward: -10.054525578934092\n",
      "15400: Done after 44 steps. Reward: 9.70006198461044\n",
      "Evaluation after 15400 episodes: -23.437707123127616 took 38.4969208240509s\n",
      "15420: Done after 174 steps. Reward: -18.9397619113116\n",
      "15440: Done after 100 steps. Reward: 8.518865827364822\n",
      "15460: Done after 35 steps. Reward: 9.158905540165629\n",
      "15480: Done after 251 steps. Reward: -26.71785815996084\n",
      "15500: Done after 100 steps. Reward: 7.544011020332257\n",
      "15520: Done after 52 steps. Reward: -14.687965519284196\n",
      "15540: Done after 172 steps. Reward: -14.815872437123883\n",
      "15560: Done after 30 steps. Reward: 9.681432509044235\n",
      "15580: Done after 52 steps. Reward: -11.896607055139198\n",
      "15600: Done after 97 steps. Reward: 6.4796447253318625\n",
      "Evaluation after 15600 episodes: -26.41141784979339 took 35.62348031997681s\n",
      "15620: Done after 251 steps. Reward: -15.072504313238696\n",
      "15640: Done after 251 steps. Reward: -10.325351514101117\n",
      "15660: Done after 60 steps. Reward: -12.284850470994254\n",
      "15680: Done after 95 steps. Reward: -18.231840943902213\n",
      "15700: Done after 57 steps. Reward: 9.548195812434313\n",
      "15720: Done after 127 steps. Reward: 8.208997671492703\n",
      "15740: Done after 33 steps. Reward: 9.630592110805972\n",
      "15760: Done after 65 steps. Reward: 9.113064061106293\n",
      "15780: Done after 58 steps. Reward: -12.291128349078333\n",
      "15800: Done after 98 steps. Reward: -12.981650809477825\n",
      "Evaluation after 15800 episodes: -26.737879390166203 took 34.93226218223572s\n",
      "15820: Done after 107 steps. Reward: 9.084084038791948\n",
      "15840: Done after 32 steps. Reward: 9.338523529323618\n",
      "15860: Done after 251 steps. Reward: -43.20604338166379\n",
      "15880: Done after 67 steps. Reward: 9.181122561074904\n",
      "15900: Done after 166 steps. Reward: -16.129269695529533\n",
      "15920: Done after 54 steps. Reward: -12.198402378557333\n",
      "15940: Done after 36 steps. Reward: 9.046643023159099\n",
      "15960: Done after 42 steps. Reward: 9.244072213427438\n",
      "15980: Done after 43 steps. Reward: 9.013019710126478\n",
      "16000: Done after 159 steps. Reward: 3.0199702749888138\n",
      "Evaluation after 16000 episodes: -20.97164802953807 took 36.98079061508179s\n",
      "16020: Done after 251 steps. Reward: -6.702760507926681\n",
      "16040: Done after 42 steps. Reward: 9.18336747215552\n",
      "16060: Done after 81 steps. Reward: -14.774334367664716\n",
      "16080: Done after 189 steps. Reward: 1.086221668096405\n",
      "16100: Done after 97 steps. Reward: 8.643111902714402\n",
      "16120: Done after 41 steps. Reward: 9.26892796795417\n",
      "16140: Done after 44 steps. Reward: 8.233878810928795\n",
      "16160: Done after 146 steps. Reward: 9.2144719587954\n",
      "16180: Done after 123 steps. Reward: -17.60215119444218\n",
      "16200: Done after 68 steps. Reward: 9.32759557533267\n",
      "Evaluation after 16200 episodes: -26.648923781845447 took 33.567479372024536s\n",
      "16220: Done after 251 steps. Reward: -44.7803585136461\n",
      "16240: Done after 108 steps. Reward: 7.660391780412021\n",
      "16260: Done after 40 steps. Reward: 9.345669077916893\n",
      "16280: Done after 251 steps. Reward: -4.706189853281787\n",
      "16300: Done after 110 steps. Reward: 9.029006237448002\n",
      "16320: Done after 64 steps. Reward: 9.361265265084\n",
      "16340: Done after 34 steps. Reward: 9.107329036176397\n",
      "16360: Done after 122 steps. Reward: -16.308570002528842\n",
      "16380: Done after 251 steps. Reward: -13.74681519037569\n",
      "16400: Done after 76 steps. Reward: -16.054411235647542\n",
      "Evaluation after 16400 episodes: -24.97715344409223 took 37.496134519577026s\n",
      "16420: Done after 41 steps. Reward: 9.570372533670021\n",
      "16440: Done after 47 steps. Reward: -11.987059157576029\n",
      "16460: Done after 49 steps. Reward: -12.595498537694963\n",
      "16480: Done after 251 steps. Reward: -3.774506178170893\n",
      "16500: Done after 46 steps. Reward: -11.657787790426124\n",
      "16520: Done after 41 steps. Reward: 9.092503309140499\n",
      "16540: Done after 108 steps. Reward: 4.236501810928979\n",
      "16560: Done after 52 steps. Reward: -11.857365747510476\n",
      "16580: Done after 38 steps. Reward: 9.53563794663362\n",
      "16600: Done after 41 steps. Reward: -11.45319013780821\n",
      "Evaluation after 16600 episodes: -24.015607628839742 took 36.43476581573486s\n",
      "16620: Done after 39 steps. Reward: 9.471543294541247\n",
      "16640: Done after 99 steps. Reward: -15.283057863688484\n",
      "16660: Done after 32 steps. Reward: 9.439115763257236\n",
      "16680: Done after 31 steps. Reward: 9.477054888468412\n",
      "16700: Done after 251 steps. Reward: -9.88697309410005\n",
      "16720: Done after 32 steps. Reward: 9.399083131321689\n",
      "16740: Done after 84 steps. Reward: 8.7535862315235\n",
      "16760: Done after 38 steps. Reward: 9.206189933224442\n",
      "16780: Done after 110 steps. Reward: -15.30021329245044\n",
      "16800: Done after 35 steps. Reward: 9.100772229396313\n",
      "Evaluation after 16800 episodes: -18.395045303375635 took 33.25098514556885s\n",
      "16820: Done after 251 steps. Reward: -10.306314374264826\n",
      "16840: Done after 49 steps. Reward: -12.45593834246256\n",
      "16860: Done after 153 steps. Reward: 5.076301901695743\n",
      "16880: Done after 105 steps. Reward: 6.832545536058334\n",
      "16900: Done after 77 steps. Reward: -12.011242206886076\n",
      "16920: Done after 251 steps. Reward: -8.876710691787018\n",
      "16940: Done after 35 steps. Reward: 9.144827384967082\n",
      "16960: Done after 32 steps. Reward: 9.464536762898343\n",
      "16980: Done after 29 steps. Reward: 9.605317540600122\n",
      "17000: Done after 32 steps. Reward: 9.396588739403075\n",
      "Evaluation after 17000 episodes: -25.526251390677107 took 36.42121195793152s\n",
      "17020: Done after 29 steps. Reward: 9.679703478321466\n",
      "17040: Done after 191 steps. Reward: -20.631593602232375\n",
      "17060: Done after 56 steps. Reward: -12.637789509837509\n",
      "17080: Done after 30 steps. Reward: 9.512479376395115\n",
      "17100: Done after 251 steps. Reward: -20.144270035767683\n",
      "17120: Done after 33 steps. Reward: 9.255325103657231\n",
      "17140: Done after 251 steps. Reward: -10.004134090920546\n",
      "17160: Done after 79 steps. Reward: 9.476640555941168\n",
      "17180: Done after 123 steps. Reward: 6.860236517546291\n",
      "17200: Done after 251 steps. Reward: -13.67243906388015\n",
      "Evaluation after 17200 episodes: -27.361361319862663 took 37.12865114212036s\n",
      "17220: Done after 50 steps. Reward: -12.39441986671362\n",
      "17240: Done after 47 steps. Reward: -12.135723479639312\n",
      "17260: Done after 62 steps. Reward: -13.67920561596545\n",
      "17280: Done after 31 steps. Reward: 9.290477176002321\n",
      "17300: Done after 59 steps. Reward: 9.308298953988436\n",
      "17320: Done after 75 steps. Reward: 9.51571093354832\n",
      "17340: Done after 251 steps. Reward: -1.0945616218297336\n",
      "17360: Done after 251 steps. Reward: -43.03411558493891\n",
      "17380: Done after 67 steps. Reward: 8.727302566183099\n",
      "17400: Done after 251 steps. Reward: -31.528382645113897\n",
      "Evaluation after 17400 episodes: -26.97772031493766 took 36.34003400802612s\n",
      "17420: Done after 67 steps. Reward: 7.68321342510232\n",
      "17440: Done after 251 steps. Reward: -36.32607763893094\n",
      "17460: Done after 30 steps. Reward: 9.662924147803574\n",
      "17480: Done after 251 steps. Reward: -20.344826851917638\n",
      "17500: Done after 31 steps. Reward: 9.375038166612109\n",
      "17520: Done after 84 steps. Reward: -13.579731671983884\n",
      "17540: Done after 62 steps. Reward: 9.02286530836796\n",
      "17560: Done after 56 steps. Reward: 9.543901464300946\n",
      "17580: Done after 61 steps. Reward: 9.004683934581202\n",
      "17600: Done after 32 steps. Reward: 9.653450508162909\n",
      "Evaluation after 17600 episodes: -19.324697160517392 took 32.74929761886597s\n",
      "17620: Done after 33 steps. Reward: 9.20086312727431\n",
      "17640: Done after 34 steps. Reward: 9.650094778793875\n",
      "17660: Done after 60 steps. Reward: -12.948522125201382\n",
      "17680: Done after 51 steps. Reward: 9.287212995404845\n",
      "17700: Done after 58 steps. Reward: 9.529434167927946\n",
      "17720: Done after 77 steps. Reward: -14.435750176339411\n",
      "17740: Done after 251 steps. Reward: -1.1191987994562738\n",
      "17760: Done after 251 steps. Reward: -17.45567813202991\n",
      "17780: Done after 39 steps. Reward: 9.194834756551815\n",
      "17800: Done after 34 steps. Reward: 9.090469367156867\n",
      "Evaluation after 17800 episodes: -28.494230158903097 took 34.527827978134155s\n",
      "17820: Done after 31 steps. Reward: 9.576402186817264\n",
      "17840: Done after 30 steps. Reward: 9.643776843791684\n",
      "17860: Done after 28 steps. Reward: 9.728819487391082\n",
      "17880: Done after 31 steps. Reward: 9.516504623575381\n",
      "17900: Done after 65 steps. Reward: 8.421702159223626\n",
      "17920: Done after 33 steps. Reward: 9.260434679159486\n",
      "17940: Done after 48 steps. Reward: -12.67009185470725\n",
      "17960: Done after 33 steps. Reward: 9.164210736248563\n",
      "17980: Done after 118 steps. Reward: 9.114698636735877\n",
      "18000: Done after 176 steps. Reward: -15.700891532467615\n",
      "Evaluation after 18000 episodes: -26.160801056896474 took 35.00061750411987s\n",
      "18020: Done after 66 steps. Reward: 9.116692339436364\n",
      "18040: Done after 63 steps. Reward: 9.270350810785784\n",
      "18060: Done after 20 steps. Reward: -10.963191662996735\n",
      "18080: Done after 109 steps. Reward: 6.902509925729977\n",
      "18100: Done after 62 steps. Reward: 9.111125420070515\n",
      "18120: Done after 32 steps. Reward: 9.490602063040571\n",
      "18140: Done after 95 steps. Reward: -14.087272722378742\n",
      "18160: Done after 154 steps. Reward: -12.759567038640423\n",
      "18180: Done after 31 steps. Reward: 9.597274828708166\n",
      "18200: Done after 51 steps. Reward: -12.927790214576287\n",
      "Evaluation after 18200 episodes: -20.671312326112478 took 32.415038108825684s\n",
      "18220: Done after 160 steps. Reward: -16.604423719216385\n",
      "18240: Done after 71 steps. Reward: -13.301709122560636\n",
      "18260: Done after 26 steps. Reward: 9.72331026116286\n",
      "18280: Done after 115 steps. Reward: -16.748713185250278\n",
      "18300: Done after 29 steps. Reward: 9.571747710145008\n",
      "18320: Done after 91 steps. Reward: 9.055638965702578\n",
      "18340: Done after 100 steps. Reward: 8.679975425517101\n",
      "18360: Done after 251 steps. Reward: -41.047201032652765\n",
      "18380: Done after 36 steps. Reward: 9.214622290000554\n",
      "18400: Done after 91 steps. Reward: -14.86420560317723\n",
      "Evaluation after 18400 episodes: -21.594434760727516 took 35.21721005439758s\n",
      "18420: Done after 33 steps. Reward: 9.37985017682278\n",
      "18440: Done after 62 steps. Reward: 8.887420372942156\n",
      "18460: Done after 61 steps. Reward: 9.273665257077372\n",
      "18480: Done after 251 steps. Reward: -36.47881358809814\n",
      "18500: Done after 251 steps. Reward: -29.570785917653012\n",
      "18520: Done after 35 steps. Reward: 8.913073344612505\n",
      "18540: Done after 29 steps. Reward: 9.571643554537959\n",
      "18560: Done after 61 steps. Reward: 9.32689977849882\n",
      "18580: Done after 41 steps. Reward: 9.056482844472093\n",
      "18600: Done after 29 steps. Reward: 9.689901795000988\n",
      "Evaluation after 18600 episodes: -25.495323948123602 took 35.44984221458435s\n",
      "18620: Done after 30 steps. Reward: 9.527312892582819\n",
      "18640: Done after 33 steps. Reward: 9.320773294374627\n",
      "18660: Done after 30 steps. Reward: 9.504520987978077\n",
      "18680: Done after 31 steps. Reward: 9.510146785353484\n",
      "18700: Done after 130 steps. Reward: 8.063113658866834\n",
      "18720: Done after 33 steps. Reward: 9.330770831879653\n",
      "18740: Done after 27 steps. Reward: 9.711950970058062\n",
      "18760: Done after 105 steps. Reward: 9.515087946582584\n",
      "18780: Done after 64 steps. Reward: 9.146617602150023\n",
      "18800: Done after 53 steps. Reward: -12.255288471621096\n",
      "Evaluation after 18800 episodes: -20.160289027057296 took 33.75481128692627s\n",
      "18820: Done after 26 steps. Reward: 9.73993797669261\n",
      "18840: Done after 93 steps. Reward: 9.03813012621101\n",
      "18860: Done after 65 steps. Reward: 9.48520114399099\n",
      "18880: Done after 251 steps. Reward: -6.9725405760154295\n",
      "18900: Done after 89 steps. Reward: -13.138536274242838\n",
      "18920: Done after 251 steps. Reward: -7.678320258011087\n",
      "18940: Done after 169 steps. Reward: 7.469599891359852\n",
      "18960: Done after 32 steps. Reward: 9.410062313788185\n",
      "18980: Done after 190 steps. Reward: -21.773823568496944\n",
      "19000: Done after 28 steps. Reward: 9.691166904476177\n",
      "Evaluation after 19000 episodes: -24.333658333471295 took 32.956040143966675s\n",
      "19020: Done after 33 steps. Reward: 9.478286028421289\n",
      "19040: Done after 34 steps. Reward: 9.164125563609808\n",
      "19060: Done after 82 steps. Reward: 9.079407939861946\n",
      "19080: Done after 209 steps. Reward: 2.637353422822927\n",
      "19100: Done after 37 steps. Reward: 9.294055339256579\n",
      "19120: Done after 60 steps. Reward: 9.472303425585643\n",
      "19140: Done after 29 steps. Reward: 9.65187515044002\n",
      "19160: Done after 65 steps. Reward: -12.574846481374232\n",
      "19180: Done after 30 steps. Reward: 9.612625514775488\n",
      "19200: Done after 156 steps. Reward: 8.704694873291261\n",
      "Evaluation after 19200 episodes: -22.503013908563716 took 33.32709217071533s\n",
      "19220: Done after 82 steps. Reward: -14.666440531341795\n",
      "19240: Done after 29 steps. Reward: 9.507717802665734\n",
      "19260: Done after 59 steps. Reward: 9.409112824332084\n",
      "19280: Done after 35 steps. Reward: 9.147140921013493\n",
      "19300: Done after 30 steps. Reward: 9.584928493745329\n",
      "19320: Done after 116 steps. Reward: 8.200690098001369\n",
      "19340: Done after 46 steps. Reward: -12.203840013152572\n",
      "19360: Done after 47 steps. Reward: 9.221724103445256\n",
      "19380: Done after 33 steps. Reward: 9.326329793728906\n",
      "19400: Done after 64 steps. Reward: 9.524927869857235\n",
      "Evaluation after 19400 episodes: -25.4757635325345 took 36.62185764312744s\n",
      "19420: Done after 64 steps. Reward: 9.159423369317395\n",
      "19440: Done after 43 steps. Reward: 9.123122229171853\n",
      "19460: Done after 75 steps. Reward: 8.866723037578431\n",
      "19480: Done after 31 steps. Reward: 9.424531550224486\n",
      "19500: Done after 32 steps. Reward: 9.39091103182579\n",
      "19520: Done after 63 steps. Reward: -12.690828388905102\n",
      "19540: Done after 33 steps. Reward: 9.455867192517275\n",
      "19560: Done after 44 steps. Reward: -11.836402916178214\n",
      "19580: Done after 31 steps. Reward: 9.453435840399354\n",
      "19600: Done after 29 steps. Reward: 9.661102957466253\n",
      "Evaluation after 19600 episodes: -18.663793339424593 took 28.46956706047058s\n",
      "19620: Done after 29 steps. Reward: 9.466832901691335\n",
      "19640: Done after 35 steps. Reward: 9.202546276469443\n",
      "19660: Done after 56 steps. Reward: -13.866346182982081\n",
      "19680: Done after 103 steps. Reward: 6.152706056994693\n",
      "19700: Done after 31 steps. Reward: 9.470667299421727\n",
      "19720: Done after 251 steps. Reward: -3.6909824948610783\n",
      "19740: Done after 251 steps. Reward: -52.576655419125046\n",
      "19760: Done after 41 steps. Reward: 9.421188047710668\n",
      "19780: Done after 55 steps. Reward: -12.863203136425755\n",
      "19800: Done after 31 steps. Reward: -11.087763348525172\n",
      "Evaluation after 19800 episodes: -25.436024921215385 took 35.43518924713135s\n",
      "19820: Done after 28 steps. Reward: 9.64032553939421\n",
      "19840: Done after 32 steps. Reward: 9.52490598313753\n",
      "19860: Done after 63 steps. Reward: 9.194939230308357\n",
      "19880: Done after 28 steps. Reward: 9.652945824772909\n",
      "19900: Done after 30 steps. Reward: 9.48466591501933\n",
      "19920: Done after 45 steps. Reward: -12.356575654692058\n",
      "19940: Done after 35 steps. Reward: 9.192348270766471\n",
      "19960: Done after 41 steps. Reward: 8.956000714255907\n",
      "19980: Done after 29 steps. Reward: 9.69045412381016\n",
      "20000: Done after 61 steps. Reward: 9.183231648036633\n",
      "Evaluation after 20000 episodes: -24.516624448026928 took 34.94267010688782s\n",
      "20020: Done after 30 steps. Reward: 9.717813665316912\n",
      "20040: Done after 251 steps. Reward: -39.96047405656972\n",
      "20060: Done after 33 steps. Reward: 9.438406898837389\n",
      "20080: Done after 29 steps. Reward: 9.599751851450876\n",
      "20100: Done after 36 steps. Reward: 8.95808342937054\n",
      "20120: Done after 250 steps. Reward: 4.807040827255844\n",
      "20140: Done after 64 steps. Reward: 8.920446826232604\n",
      "20160: Done after 81 steps. Reward: -12.00490058038521\n",
      "20180: Done after 28 steps. Reward: 9.654767963254372\n",
      "20200: Done after 48 steps. Reward: -12.165214784608729\n",
      "Evaluation after 20200 episodes: -17.71143258317512 took 31.446496725082397s\n",
      "20220: Done after 34 steps. Reward: 9.230574711642612\n",
      "20240: Done after 122 steps. Reward: -17.02677304621693\n",
      "20260: Done after 251 steps. Reward: -19.89933272253028\n",
      "20280: Done after 29 steps. Reward: 9.694971919682047\n",
      "20300: Done after 29 steps. Reward: 9.65434945800491\n",
      "20320: Done after 40 steps. Reward: 9.137375986620034\n",
      "20340: Done after 31 steps. Reward: 9.420631849446846\n",
      "20360: Done after 33 steps. Reward: 9.077265747027383\n",
      "20380: Done after 75 steps. Reward: 7.774104623560481\n",
      "20400: Done after 31 steps. Reward: 9.411423783212138\n",
      "Evaluation after 20400 episodes: -19.692944114827814 took 35.833606243133545s\n",
      "20420: Done after 65 steps. Reward: 9.35323655287564\n",
      "20440: Done after 55 steps. Reward: -15.95579958961595\n",
      "20460: Done after 30 steps. Reward: 9.539703238493434\n",
      "20480: Done after 33 steps. Reward: 9.028185665053444\n",
      "20500: Done after 31 steps. Reward: 9.425308440608145\n",
      "20520: Done after 64 steps. Reward: 9.036706497620857\n",
      "20540: Done after 251 steps. Reward: -39.06822177623012\n",
      "20560: Done after 36 steps. Reward: 9.163883512682098\n",
      "20580: Done after 35 steps. Reward: 8.99405392463477\n",
      "20600: Done after 47 steps. Reward: -12.141052864001919\n",
      "Evaluation after 20600 episodes: -20.090218364345585 took 33.333332538604736s\n",
      "20620: Done after 251 steps. Reward: -43.125209281907416\n",
      "20640: Done after 31 steps. Reward: 9.483465992050823\n",
      "20660: Done after 31 steps. Reward: 9.423417040307132\n",
      "20680: Done after 32 steps. Reward: 9.32013433493092\n",
      "20700: Done after 30 steps. Reward: 9.697762710430949\n",
      "20720: Done after 63 steps. Reward: 8.92879119441325\n",
      "20740: Done after 33 steps. Reward: 9.205324004940898\n",
      "20760: Done after 222 steps. Reward: 1.5029682932533923\n",
      "20780: Done after 77 steps. Reward: -12.331543564511422\n",
      "20800: Done after 51 steps. Reward: -13.661740642242023\n",
      "Evaluation after 20800 episodes: -20.948919469455856 took 33.95731067657471s\n",
      "20820: Done after 37 steps. Reward: 9.086281775359291\n",
      "20840: Done after 78 steps. Reward: -11.840307966124413\n",
      "20860: Done after 101 steps. Reward: -12.627811250702699\n",
      "20880: Done after 32 steps. Reward: 9.373713919019826\n",
      "20900: Done after 31 steps. Reward: 9.496238934506543\n",
      "20920: Done after 31 steps. Reward: 9.426411452387311\n",
      "20940: Done after 151 steps. Reward: 6.826948398748204\n",
      "20960: Done after 63 steps. Reward: 9.174700935526545\n",
      "20980: Done after 66 steps. Reward: -13.864491658181834\n",
      "21000: Done after 33 steps. Reward: 9.195241985600484\n",
      "Evaluation after 21000 episodes: -15.015792188395697 took 36.06843852996826s\n",
      "21020: Done after 31 steps. Reward: 9.58222209580721\n",
      "21040: Done after 33 steps. Reward: 9.002553389676232\n",
      "21060: Done after 30 steps. Reward: 9.616819261958673\n",
      "21080: Done after 179 steps. Reward: 0.9233278164187322\n",
      "21100: Done after 251 steps. Reward: -6.559565441313457\n",
      "21120: Done after 33 steps. Reward: 9.349663425652777\n",
      "21140: Done after 34 steps. Reward: 9.113998880581901\n",
      "21160: Done after 51 steps. Reward: -12.398946831508837\n",
      "21180: Done after 31 steps. Reward: 9.465739426796002\n",
      "21200: Done after 139 steps. Reward: 5.9625256049876345\n",
      "Evaluation after 21200 episodes: -15.110731158673842 took 33.06479525566101s\n",
      "21220: Done after 70 steps. Reward: 8.93970821602032\n",
      "21240: Done after 249 steps. Reward: 1.110010799479113\n",
      "21260: Done after 91 steps. Reward: -15.23175505170633\n",
      "21280: Done after 103 steps. Reward: 6.392646035387281\n",
      "21300: Done after 35 steps. Reward: 9.216518828767386\n",
      "21320: Done after 61 steps. Reward: 8.880691792962793\n",
      "21340: Done after 251 steps. Reward: -37.79304701032147\n",
      "21360: Done after 32 steps. Reward: 9.161942787940527\n",
      "21380: Done after 51 steps. Reward: -12.885957908474325\n",
      "21400: Done after 63 steps. Reward: 9.205692526449411\n",
      "Evaluation after 21400 episodes: -17.904303641129392 took 35.37599754333496s\n",
      "21420: Done after 249 steps. Reward: 1.913052156825259\n",
      "21440: Done after 72 steps. Reward: 9.159738381003375\n",
      "21460: Done after 55 steps. Reward: -12.902947160581625\n",
      "21480: Done after 63 steps. Reward: 9.293698684266296\n",
      "21500: Done after 251 steps. Reward: -35.068386151183304\n",
      "21520: Done after 83 steps. Reward: -13.242198778561626\n",
      "21540: Done after 67 steps. Reward: 8.950963061826654\n",
      "21560: Done after 198 steps. Reward: 4.240893585299056\n",
      "21580: Done after 64 steps. Reward: 9.267477313080333\n",
      "21600: Done after 44 steps. Reward: -11.841107540056766\n",
      "Evaluation after 21600 episodes: -26.400585589669454 took 36.8966109752655s\n",
      "21620: Done after 117 steps. Reward: -13.567598120588436\n",
      "21640: Done after 34 steps. Reward: 9.215853165300087\n",
      "21660: Done after 35 steps. Reward: 9.21872042327002\n",
      "21680: Done after 47 steps. Reward: -11.81241278935795\n",
      "21700: Done after 29 steps. Reward: 9.633184567699423\n",
      "21720: Done after 54 steps. Reward: -14.203520746325555\n",
      "21740: Done after 29 steps. Reward: 9.574428816700456\n",
      "21760: Done after 30 steps. Reward: 9.473811530860878\n",
      "21780: Done after 188 steps. Reward: -17.337426161684792\n",
      "21800: Done after 119 steps. Reward: 8.961151507190689\n",
      "Evaluation after 21800 episodes: -16.677762704002706 took 34.78129744529724s\n",
      "21820: Done after 66 steps. Reward: 8.844012691865187\n",
      "21840: Done after 58 steps. Reward: 9.349377636984533\n",
      "21860: Done after 32 steps. Reward: 9.41661069452594\n",
      "21880: Done after 31 steps. Reward: 9.335134199667968\n",
      "21900: Done after 68 steps. Reward: 8.800817096445334\n",
      "21920: Done after 59 steps. Reward: 9.471855115275089\n",
      "21940: Done after 49 steps. Reward: -12.341163952088703\n",
      "21960: Done after 31 steps. Reward: 9.368212064917097\n",
      "21980: Done after 30 steps. Reward: 9.44912063686629\n",
      "22000: Done after 251 steps. Reward: -39.15417500046566\n",
      "Evaluation after 22000 episodes: -23.723442484496413 took 32.19843888282776s\n",
      "22020: Done after 42 steps. Reward: -11.748380214054938\n",
      "22040: Done after 58 steps. Reward: 9.298961409901503\n",
      "22060: Done after 111 steps. Reward: 7.487366829683205\n",
      "22080: Done after 112 steps. Reward: -14.871938236855849\n",
      "22100: Done after 30 steps. Reward: 9.594242498117682\n",
      "22120: Done after 88 steps. Reward: 9.158073613373077\n",
      "22140: Done after 251 steps. Reward: -52.74670910390313\n",
      "22160: Done after 27 steps. Reward: 9.714838007848213\n",
      "22180: Done after 34 steps. Reward: 9.16770492532659\n",
      "22200: Done after 51 steps. Reward: -12.271012364030138\n",
      "Evaluation after 22200 episodes: -13.767364558862822 took 30.73018479347229s\n",
      "22220: Done after 33 steps. Reward: 9.149387043032483\n",
      "22240: Done after 124 steps. Reward: 8.626224069936445\n",
      "22260: Done after 30 steps. Reward: 9.635368197813792\n",
      "22280: Done after 36 steps. Reward: 9.032153455634925\n",
      "22300: Done after 31 steps. Reward: 9.351505944207505\n",
      "22320: Done after 66 steps. Reward: 8.775002693791645\n",
      "22340: Done after 30 steps. Reward: 9.38400646289654\n",
      "22360: Done after 251 steps. Reward: -2.139384978016502\n",
      "22380: Done after 32 steps. Reward: 9.461915832362877\n",
      "22400: Done after 81 steps. Reward: -12.059460796278909\n",
      "Evaluation after 22400 episodes: -18.02760209347912 took 32.72007369995117s\n",
      "22420: Done after 31 steps. Reward: 9.459984474251268\n",
      "22440: Done after 31 steps. Reward: 9.363317792713929\n",
      "22460: Done after 186 steps. Reward: 8.453710920743394\n",
      "22480: Done after 47 steps. Reward: -12.553681169199692\n",
      "22500: Done after 29 steps. Reward: 9.725255659182174\n",
      "22520: Done after 135 steps. Reward: -19.976481751428857\n",
      "22540: Done after 121 steps. Reward: 9.206128335187898\n",
      "22560: Done after 27 steps. Reward: 9.744901019431017\n",
      "22580: Done after 251 steps. Reward: -14.933280277702238\n",
      "22600: Done after 34 steps. Reward: 9.10604511480926\n",
      "Evaluation after 22600 episodes: -16.851696053100298 took 35.61854410171509s\n",
      "22620: Done after 28 steps. Reward: 9.682906211637926\n",
      "22640: Done after 251 steps. Reward: -10.801047205540508\n",
      "22660: Done after 36 steps. Reward: 8.900171766504364\n",
      "22680: Done after 32 steps. Reward: 9.411867774755715\n",
      "22700: Done after 60 steps. Reward: 9.49933627719734\n",
      "22720: Done after 100 steps. Reward: 8.988556081115181\n",
      "22740: Done after 194 steps. Reward: 5.906558659439696\n",
      "22760: Done after 82 steps. Reward: -13.016705603071284\n",
      "22780: Done after 41 steps. Reward: 9.238575362631714\n",
      "22800: Done after 55 steps. Reward: -12.42835027859805\n",
      "Evaluation after 22800 episodes: -16.883638678302805 took 38.18596601486206s\n",
      "22820: Done after 26 steps. Reward: 9.703693680557443\n",
      "22840: Done after 31 steps. Reward: 9.502881887077745\n",
      "22860: Done after 34 steps. Reward: 9.266146491356606\n",
      "22880: Done after 61 steps. Reward: 9.316061724654343\n",
      "22900: Done after 111 steps. Reward: 6.516582471398531\n",
      "22920: Done after 30 steps. Reward: 9.503765995284002\n",
      "22940: Done after 78 steps. Reward: -12.763963400084336\n",
      "22960: Done after 251 steps. Reward: -6.154374768139776\n",
      "22980: Done after 115 steps. Reward: -15.54314310608612\n",
      "23000: Done after 67 steps. Reward: -13.040478114136114\n",
      "Evaluation after 23000 episodes: -19.547344680156897 took 33.12458086013794s\n",
      "23020: Done after 36 steps. Reward: 8.868005335931963\n",
      "23040: Done after 102 steps. Reward: -12.103850860823625\n",
      "23060: Done after 251 steps. Reward: -44.342105578996836\n",
      "23080: Done after 251 steps. Reward: -33.95181207286826\n",
      "23100: Done after 36 steps. Reward: 8.985717493244238\n",
      "23120: Done after 32 steps. Reward: 9.595512932263004\n",
      "23140: Done after 251 steps. Reward: -26.309957094665382\n",
      "23160: Done after 251 steps. Reward: -7.796407282127003\n",
      "23180: Done after 251 steps. Reward: -21.77764342471899\n",
      "23200: Done after 30 steps. Reward: 9.602415365848877\n",
      "Evaluation after 23200 episodes: -13.228324753164665 took 28.557295560836792s\n",
      "23220: Done after 82 steps. Reward: -12.855712489585686\n",
      "23240: Done after 33 steps. Reward: 9.240400877967025\n",
      "23260: Done after 116 steps. Reward: -14.602626247131473\n",
      "23280: Done after 33 steps. Reward: 9.143910142169892\n",
      "23300: Done after 251 steps. Reward: -26.80775265293373\n",
      "23320: Done after 31 steps. Reward: 9.290773277872276\n",
      "23340: Done after 35 steps. Reward: 8.99217034271421\n",
      "23360: Done after 74 steps. Reward: 6.315830038226794\n",
      "23380: Done after 35 steps. Reward: 9.140496842571814\n",
      "23400: Done after 251 steps. Reward: -41.08785590518886\n",
      "Evaluation after 23400 episodes: -13.908425738226784 took 32.31942319869995s\n",
      "23420: Done after 94 steps. Reward: -13.782817671568763\n",
      "23440: Done after 33 steps. Reward: 9.385125911928824\n",
      "23460: Done after 46 steps. Reward: -11.638005615605284\n",
      "23480: Done after 60 steps. Reward: 9.223476164418628\n",
      "23500: Done after 56 steps. Reward: -12.902720329627558\n",
      "23520: Done after 36 steps. Reward: 8.994014270569064\n",
      "23540: Done after 69 steps. Reward: 8.095643485544961\n",
      "23560: Done after 48 steps. Reward: -12.057890052747965\n",
      "23580: Done after 31 steps. Reward: 9.545525528163646\n",
      "23600: Done after 33 steps. Reward: 9.316264849026297\n",
      "Evaluation after 23600 episodes: -16.419323966400682 took 33.68935227394104s\n",
      "23620: Done after 26 steps. Reward: 9.673374086886437\n",
      "23640: Done after 29 steps. Reward: 9.624641782276619\n",
      "23660: Done after 119 steps. Reward: -14.27593651022443\n",
      "23680: Done after 31 steps. Reward: 9.399185096663974\n",
      "23700: Done after 96 steps. Reward: 9.012564262725094\n",
      "23720: Done after 126 steps. Reward: -26.332675306820345\n",
      "23740: Done after 33 steps. Reward: 9.266606226041898\n",
      "23760: Done after 29 steps. Reward: 9.710911412472493\n",
      "23780: Done after 57 steps. Reward: 9.484505955315703\n",
      "23800: Done after 31 steps. Reward: 9.47716926194049\n",
      "Evaluation after 23800 episodes: -20.59923021474485 took 33.82732677459717s\n",
      "23820: Done after 31 steps. Reward: 9.344001795930986\n",
      "23840: Done after 29 steps. Reward: 9.610366043574155\n",
      "23860: Done after 28 steps. Reward: 9.721868696625634\n",
      "23880: Done after 30 steps. Reward: 9.519792112073793\n",
      "23900: Done after 45 steps. Reward: -12.301898924169794\n",
      "23920: Done after 83 steps. Reward: -13.456786372572278\n",
      "23940: Done after 100 steps. Reward: 6.256572181787611\n",
      "23960: Done after 243 steps. Reward: -2.617210198109335\n",
      "23980: Done after 33 steps. Reward: 9.235897237588018\n",
      "24000: Done after 30 steps. Reward: 9.61306793714578\n",
      "Evaluation after 24000 episodes: -16.122303581768573 took 33.03366780281067s\n",
      "24020: Done after 251 steps. Reward: -16.54630799486043\n",
      "24040: Done after 36 steps. Reward: 8.977473964247164\n",
      "24060: Done after 61 steps. Reward: 9.164462689548499\n",
      "24080: Done after 35 steps. Reward: 8.914396333945303\n",
      "24100: Done after 31 steps. Reward: 9.4473968949634\n",
      "24120: Done after 70 steps. Reward: -15.010799753416407\n",
      "24140: Done after 104 steps. Reward: 8.093019640195374\n",
      "24160: Done after 60 steps. Reward: 9.683970087065081\n",
      "24180: Done after 34 steps. Reward: 9.118188606742686\n",
      "24200: Done after 35 steps. Reward: 8.882080230541899\n",
      "Evaluation after 24200 episodes: -25.430604891902064 took 33.5058696269989s\n",
      "24220: Done after 41 steps. Reward: -11.753659195789231\n",
      "24240: Done after 33 steps. Reward: 9.211776286674342\n",
      "24260: Done after 31 steps. Reward: 9.44775268670514\n",
      "24280: Done after 251 steps. Reward: -13.117657758156476\n",
      "24300: Done after 189 steps. Reward: -24.34107431535164\n",
      "24320: Done after 251 steps. Reward: -43.721106256959004\n",
      "24340: Done after 32 steps. Reward: 9.264677104864655\n",
      "24360: Done after 251 steps. Reward: -17.68189150384747\n",
      "24380: Done after 35 steps. Reward: 9.133551979449352\n",
      "24400: Done after 251 steps. Reward: -31.19440583099759\n",
      "Evaluation after 24400 episodes: -18.7956757560039 took 38.870110750198364s\n",
      "24420: Done after 118 steps. Reward: -14.359268142267588\n",
      "24440: Done after 71 steps. Reward: -14.642719216634685\n",
      "24460: Done after 37 steps. Reward: 8.76111410533736\n",
      "24480: Done after 32 steps. Reward: 9.353302376068088\n",
      "24500: Done after 31 steps. Reward: 9.489697487937985\n",
      "24520: Done after 55 steps. Reward: -12.988624293000868\n",
      "24540: Done after 50 steps. Reward: 7.014383286005806\n",
      "24560: Done after 45 steps. Reward: -11.694163927918439\n",
      "24580: Done after 31 steps. Reward: 9.60029466325718\n",
      "24600: Done after 29 steps. Reward: 9.566552973507564\n",
      "Evaluation after 24600 episodes: -25.517112019543287 took 32.405012130737305s\n",
      "24620: Done after 251 steps. Reward: -5.819238110632567\n",
      "24640: Done after 251 steps. Reward: -27.42201435109811\n",
      "24660: Done after 31 steps. Reward: 9.618579373298767\n",
      "24680: Done after 31 steps. Reward: 9.509668782567163\n",
      "24700: Done after 32 steps. Reward: 9.400293852446481\n",
      "24720: Done after 31 steps. Reward: 9.592752423018391\n",
      "24740: Done after 59 steps. Reward: 9.327736390684517\n",
      "24760: Done after 28 steps. Reward: 9.641495464585054\n",
      "24780: Done after 61 steps. Reward: 9.121032956709653\n",
      "24800: Done after 32 steps. Reward: 9.411585884003465\n",
      "Evaluation after 24800 episodes: -17.52291510788001 took 33.40129733085632s\n",
      "24820: Done after 251 steps. Reward: -25.681245255781448\n",
      "24840: Done after 58 steps. Reward: 9.358696626403177\n",
      "24860: Done after 251 steps. Reward: -31.72000154376284\n",
      "24880: Done after 34 steps. Reward: 9.16764915180045\n",
      "24900: Done after 83 steps. Reward: -12.601995584691094\n",
      "24920: Done after 43 steps. Reward: -11.665662231816812\n",
      "24940: Done after 60 steps. Reward: 9.094857341457843\n",
      "24960: Done after 29 steps. Reward: 9.601929126914882\n",
      "24980: Done after 33 steps. Reward: 9.21922882716764\n",
      "25000: Done after 61 steps. Reward: -12.79462008356116\n",
      "Evaluation after 25000 episodes: -20.83316986710613 took 34.3807327747345s\n",
      "25020: Done after 58 steps. Reward: 9.560868247869838\n",
      "25040: Done after 33 steps. Reward: 9.178868915445488\n",
      "25060: Done after 31 steps. Reward: 9.329950475023816\n",
      "25080: Done after 36 steps. Reward: 9.61827764992486\n",
      "25100: Done after 60 steps. Reward: 9.055064744446693\n",
      "25120: Done after 82 steps. Reward: 7.423257416782237\n",
      "25140: Done after 61 steps. Reward: 9.651940906310656\n",
      "25160: Done after 45 steps. Reward: -11.639198255621798\n",
      "25180: Done after 110 steps. Reward: 6.566891463569687\n",
      "25200: Done after 29 steps. Reward: 9.59768499770168\n",
      "Evaluation after 25200 episodes: -24.228311832328085 took 34.8043429851532s\n",
      "25220: Done after 29 steps. Reward: 9.655839960373635\n",
      "25240: Done after 30 steps. Reward: 9.553056515094491\n",
      "25260: Done after 251 steps. Reward: -37.91146481453023\n",
      "25280: Done after 34 steps. Reward: 9.368968834422846\n",
      "25300: Done after 62 steps. Reward: 9.624490210046693\n",
      "25320: Done after 31 steps. Reward: 9.468956902883512\n",
      "25340: Done after 29 steps. Reward: 9.658607189254392\n",
      "25360: Done after 43 steps. Reward: 8.209600706293651\n",
      "25380: Done after 251 steps. Reward: -39.55337631704796\n",
      "25400: Done after 34 steps. Reward: 9.142979605708643\n",
      "Evaluation after 25400 episodes: -25.635696308592188 took 35.97314190864563s\n",
      "25420: Done after 251 steps. Reward: -0.746296212919807\n",
      "25440: Done after 81 steps. Reward: 8.398471506132704\n",
      "25460: Done after 30 steps. Reward: 9.431466816916515\n",
      "25480: Done after 61 steps. Reward: 9.372713507617936\n",
      "25500: Done after 31 steps. Reward: 9.380871714573791\n",
      "25520: Done after 30 steps. Reward: 9.781302049421422\n",
      "25540: Done after 58 steps. Reward: 9.365584976065504\n",
      "25560: Done after 175 steps. Reward: -16.342589421987988\n",
      "25580: Done after 251 steps. Reward: -12.409140273666507\n",
      "25600: Done after 54 steps. Reward: -12.555670274043965\n",
      "Evaluation after 25600 episodes: -21.71976514491238 took 37.612940073013306s\n",
      "25620: Done after 41 steps. Reward: -11.550179622293296\n",
      "25640: Done after 34 steps. Reward: 9.172959020653028\n",
      "25660: Done after 31 steps. Reward: 9.244846405703905\n",
      "25680: Done after 251 steps. Reward: -42.56615588063077\n",
      "25700: Done after 34 steps. Reward: 9.108831320860688\n",
      "25720: Done after 34 steps. Reward: 9.304946267127598\n",
      "25740: Done after 251 steps. Reward: -33.23051439463073\n",
      "25760: Done after 32 steps. Reward: 9.44987968416899\n",
      "25780: Done after 31 steps. Reward: 9.621100110992769\n",
      "25800: Done after 30 steps. Reward: 9.397846005113946\n",
      "Evaluation after 25800 episodes: -25.297358851052405 took 33.16385245323181s\n",
      "25820: Done after 251 steps. Reward: -47.82017659931661\n",
      "25840: Done after 60 steps. Reward: 9.477934014498052\n",
      "25860: Done after 251 steps. Reward: -49.34328990548235\n",
      "25880: Done after 61 steps. Reward: 9.179304754203933\n",
      "25900: Done after 33 steps. Reward: 9.279600378508661\n",
      "25920: Done after 32 steps. Reward: 9.562179834800336\n",
      "25940: Done after 251 steps. Reward: -43.36123958923943\n",
      "25960: Done after 34 steps. Reward: 9.013337213728636\n",
      "25980: Done after 31 steps. Reward: 9.34498959653876\n",
      "26000: Done after 251 steps. Reward: -10.884312365538804\n",
      "Evaluation after 26000 episodes: -21.188512777224172 took 34.69019079208374s\n",
      "26020: Done after 59 steps. Reward: 9.12487575854421\n",
      "26040: Done after 30 steps. Reward: 9.417495702246791\n",
      "26060: Done after 32 steps. Reward: 9.295990171946125\n",
      "26080: Done after 251 steps. Reward: -7.04091669429412\n",
      "26100: Done after 49 steps. Reward: -12.998973256683625\n",
      "26120: Done after 32 steps. Reward: 9.379531521022676\n",
      "26140: Done after 32 steps. Reward: 9.152961638959916\n",
      "26160: Done after 37 steps. Reward: 9.04148337940377\n",
      "26180: Done after 33 steps. Reward: 9.191273507979343\n",
      "26200: Done after 54 steps. Reward: -13.955416919717711\n",
      "Evaluation after 26200 episodes: -19.667700305931263 took 36.954468727111816s\n",
      "26220: Done after 34 steps. Reward: 9.29160530958722\n",
      "26240: Done after 29 steps. Reward: 9.578045694653248\n",
      "26260: Done after 30 steps. Reward: 9.499096273626664\n",
      "26280: Done after 32 steps. Reward: 9.51653263030747\n",
      "26300: Done after 32 steps. Reward: 9.414213408001936\n",
      "26320: Done after 32 steps. Reward: 9.13302145564838\n",
      "26340: Done after 31 steps. Reward: 9.399765930634452\n",
      "26360: Done after 31 steps. Reward: 9.385337837589592\n",
      "26380: Done after 251 steps. Reward: -34.923897557860904\n",
      "26400: Done after 31 steps. Reward: 9.614406857567793\n",
      "Evaluation after 26400 episodes: -25.891645228240172 took 36.64285969734192s\n",
      "26420: Done after 32 steps. Reward: 9.202497756521339\n",
      "26440: Done after 29 steps. Reward: 9.679490624301494\n",
      "26460: Done after 251 steps. Reward: -38.454036132311465\n",
      "26480: Done after 93 steps. Reward: 8.801556448205883\n",
      "26500: Done after 30 steps. Reward: 9.651017589092874\n",
      "26520: Done after 77 steps. Reward: -14.346675190282\n",
      "26540: Done after 61 steps. Reward: -12.252787623454516\n",
      "26560: Done after 30 steps. Reward: 9.59761541671203\n",
      "26580: Done after 51 steps. Reward: -12.455382124524938\n",
      "26600: Done after 139 steps. Reward: -15.247813609999096\n",
      "Evaluation after 26600 episodes: -20.302723674969876 took 38.856969118118286s\n",
      "26620: Done after 251 steps. Reward: -0.6926159723402406\n",
      "26640: Done after 32 steps. Reward: 9.469221787149369\n",
      "26660: Done after 28 steps. Reward: 9.694969994429798\n",
      "26680: Done after 85 steps. Reward: -14.238821624455081\n",
      "26700: Done after 33 steps. Reward: 9.43779015628981\n",
      "26720: Done after 81 steps. Reward: -12.996847007803881\n",
      "26740: Done after 57 steps. Reward: 9.231770012206354\n",
      "26760: Done after 32 steps. Reward: 9.585157027785772\n",
      "26780: Done after 36 steps. Reward: 9.045502370592338\n",
      "26800: Done after 34 steps. Reward: 9.093195893440729\n",
      "Evaluation after 26800 episodes: -19.988512475184212 took 36.041523933410645s\n",
      "26820: Done after 51 steps. Reward: -12.565488730135986\n",
      "26840: Done after 34 steps. Reward: 9.272732311213074\n",
      "26860: Done after 34 steps. Reward: 8.988520866877913\n",
      "26880: Done after 71 steps. Reward: 7.789967796763788\n",
      "26900: Done after 30 steps. Reward: 9.545973099690451\n",
      "26920: Done after 84 steps. Reward: -13.106171294801346\n",
      "26940: Done after 251 steps. Reward: -38.45217889512565\n",
      "26960: Done after 30 steps. Reward: 9.520540362757611\n",
      "26980: Done after 29 steps. Reward: 9.627879198689072\n",
      "27000: Done after 251 steps. Reward: -20.48916773871347\n",
      "Evaluation after 27000 episodes: -22.753754894484178 took 35.628029108047485s\n",
      "27020: Done after 246 steps. Reward: -22.017123919480706\n",
      "27040: Done after 29 steps. Reward: 9.709963803372744\n",
      "27060: Done after 64 steps. Reward: 9.438158780897266\n",
      "27080: Done after 84 steps. Reward: -14.806544895643519\n",
      "27100: Done after 33 steps. Reward: 9.281293872439663\n",
      "27120: Done after 34 steps. Reward: 9.393790348920588\n",
      "27140: Done after 32 steps. Reward: 9.172912426807368\n",
      "27160: Done after 27 steps. Reward: 9.687257071817838\n",
      "27180: Done after 31 steps. Reward: 9.386074205212108\n",
      "27200: Done after 52 steps. Reward: -12.163818120673902\n",
      "Evaluation after 27200 episodes: -21.53013435585153 took 34.104153871536255s\n",
      "27220: Done after 31 steps. Reward: 9.598882403171812\n",
      "27240: Done after 30 steps. Reward: 9.515172609199276\n",
      "27260: Done after 92 steps. Reward: -13.92672382236487\n",
      "27280: Done after 36 steps. Reward: 8.958011870371756\n",
      "27300: Done after 115 steps. Reward: 7.472665058399983\n",
      "27320: Done after 62 steps. Reward: 8.87179562818838\n",
      "27340: Done after 67 steps. Reward: 8.956217505795209\n",
      "27360: Done after 31 steps. Reward: 9.369725324020099\n",
      "27380: Done after 33 steps. Reward: 9.288093477270536\n",
      "27400: Done after 33 steps. Reward: 9.227674406832003\n",
      "Evaluation after 27400 episodes: -17.67523796904145 took 33.49117636680603s\n",
      "27420: Done after 33 steps. Reward: 9.160595423822805\n",
      "27440: Done after 31 steps. Reward: 9.446450324624896\n",
      "27460: Done after 30 steps. Reward: 9.578392388023651\n",
      "27480: Done after 54 steps. Reward: -12.732889670454664\n",
      "27500: Done after 251 steps. Reward: -12.932292797510494\n",
      "27520: Done after 234 steps. Reward: 1.0578750462066147\n",
      "27540: Done after 78 steps. Reward: -14.372908625509307\n",
      "27560: Done after 34 steps. Reward: 9.13635903881512\n",
      "27580: Done after 30 steps. Reward: 9.470602517852186\n",
      "27600: Done after 32 steps. Reward: 9.324887568390235\n",
      "Evaluation after 27600 episodes: -19.65608255603793 took 34.26059579849243s\n",
      "27620: Done after 31 steps. Reward: 9.347748333824475\n",
      "27640: Done after 251 steps. Reward: -0.5393604272259512\n",
      "27660: Done after 34 steps. Reward: 9.194480669269216\n",
      "27680: Done after 29 steps. Reward: 9.607682187254238\n",
      "27700: Done after 83 steps. Reward: -13.856257900209703\n",
      "27720: Done after 35 steps. Reward: 9.089334631924023\n",
      "27740: Done after 31 steps. Reward: 9.520709732306816\n",
      "27760: Done after 62 steps. Reward: 8.988191390000651\n",
      "27780: Done after 62 steps. Reward: 8.83094945876272\n",
      "27800: Done after 33 steps. Reward: 9.452676190309308\n",
      "Evaluation after 27800 episodes: -24.850855301551686 took 38.72288632392883s\n",
      "27820: Done after 31 steps. Reward: 9.331495098510551\n",
      "27840: Done after 31 steps. Reward: 9.496685451731087\n",
      "27860: Done after 29 steps. Reward: 9.508921194975164\n",
      "27880: Done after 251 steps. Reward: -19.321124592632557\n",
      "27900: Done after 33 steps. Reward: 9.198336190946943\n",
      "27920: Done after 251 steps. Reward: -44.38399654017779\n",
      "27940: Done after 44 steps. Reward: -11.996463090226623\n",
      "27960: Done after 251 steps. Reward: -0.8874880980609814\n",
      "27980: Done after 126 steps. Reward: -12.481016646742635\n",
      "28000: Done after 31 steps. Reward: 9.507791451857976\n",
      "Evaluation after 28000 episodes: -25.098896633056846 took 37.14118266105652s\n",
      "28020: Done after 32 steps. Reward: 9.484687860019845\n",
      "28040: Done after 62 steps. Reward: 9.4401106063162\n",
      "28060: Done after 89 steps. Reward: -13.024534580381516\n",
      "28080: Done after 101 steps. Reward: -15.579371560321377\n",
      "28100: Done after 60 steps. Reward: 8.927422486795793\n",
      "28120: Done after 30 steps. Reward: 9.50264713472026\n",
      "28140: Done after 31 steps. Reward: 9.511812739931639\n",
      "28160: Done after 99 steps. Reward: 9.213165994109428\n",
      "28180: Done after 171 steps. Reward: -19.347711130854023\n",
      "28200: Done after 68 steps. Reward: 8.969338735495647\n",
      "Evaluation after 28200 episodes: -16.314284389422067 took 34.15433716773987s\n",
      "28220: Done after 29 steps. Reward: 9.58141846289956\n",
      "28240: Done after 30 steps. Reward: 9.720310683990744\n",
      "28260: Done after 64 steps. Reward: 9.03592301871215\n",
      "28280: Done after 62 steps. Reward: 9.35298133876113\n",
      "28300: Done after 33 steps. Reward: 9.354587883354416\n",
      "28320: Done after 28 steps. Reward: 9.682518077443023\n",
      "28340: Done after 251 steps. Reward: -18.844157256762013\n",
      "28360: Done after 60 steps. Reward: 9.269709662880762\n",
      "28380: Done after 64 steps. Reward: 9.25271027725042\n",
      "28400: Done after 223 steps. Reward: 4.049855181006427\n",
      "Evaluation after 28400 episodes: -22.675035076797634 took 34.327616453170776s\n",
      "28420: Done after 31 steps. Reward: 9.27744857430065\n",
      "28440: Done after 29 steps. Reward: 9.627406495510748\n",
      "28460: Done after 28 steps. Reward: 9.594026340477823\n",
      "28480: Done after 46 steps. Reward: -11.750851895595293\n",
      "28500: Done after 64 steps. Reward: 8.936399217318352\n",
      "28520: Done after 28 steps. Reward: 9.71142665667675\n",
      "28540: Done after 251 steps. Reward: -41.92454110973763\n",
      "28560: Done after 43 steps. Reward: -11.357783319020669\n",
      "28580: Done after 251 steps. Reward: -47.18815424756267\n",
      "28600: Done after 64 steps. Reward: 8.446389658755153\n",
      "Evaluation after 28600 episodes: -22.848740894590968 took 35.335360527038574s\n",
      "28620: Done after 32 steps. Reward: 9.463444693035182\n",
      "28640: Done after 32 steps. Reward: 9.411693476178453\n",
      "28660: Done after 29 steps. Reward: 9.566490882949893\n",
      "28680: Done after 36 steps. Reward: 9.251264600688321\n",
      "28700: Done after 62 steps. Reward: 8.792259868332788\n",
      "28720: Done after 34 steps. Reward: 8.879953596528471\n",
      "28740: Done after 47 steps. Reward: -12.121218344157477\n",
      "28760: Done after 61 steps. Reward: 9.0493664838092\n",
      "28780: Done after 31 steps. Reward: 9.368008194891775\n",
      "28800: Done after 32 steps. Reward: 9.443250163560062\n",
      "Evaluation after 28800 episodes: -25.61848149203272 took 33.984904050827026s\n",
      "28820: Done after 59 steps. Reward: 9.500200797629786\n",
      "28840: Done after 251 steps. Reward: -43.07292053101605\n",
      "28860: Done after 251 steps. Reward: -45.575478348334805\n",
      "28880: Done after 36 steps. Reward: 8.924849774841986\n",
      "28900: Done after 30 steps. Reward: 9.663974435330882\n",
      "28920: Done after 31 steps. Reward: 9.527153528475779\n",
      "28940: Done after 103 steps. Reward: -14.565622922105362\n",
      "28960: Done after 29 steps. Reward: 9.518010102054351\n",
      "28980: Done after 245 steps. Reward: 6.368091112085404\n",
      "29000: Done after 30 steps. Reward: 9.655239734665168\n",
      "Evaluation after 29000 episodes: -13.87190080048416 took 28.9378604888916s\n",
      "29020: Done after 31 steps. Reward: 9.366343276781715\n",
      "29040: Done after 50 steps. Reward: -12.65680926245069\n",
      "29060: Done after 29 steps. Reward: 9.649005377788455\n",
      "29080: Done after 27 steps. Reward: 9.629527343909265\n",
      "29100: Done after 115 steps. Reward: 7.2393053802560665\n",
      "29120: Done after 30 steps. Reward: 9.445322061432313\n",
      "29140: Done after 51 steps. Reward: -13.507521588845936\n",
      "29160: Done after 30 steps. Reward: 9.572496360594736\n",
      "29180: Done after 56 steps. Reward: -14.076832863487912\n",
      "29200: Done after 57 steps. Reward: -13.12631423619658\n",
      "Evaluation after 29200 episodes: -14.570673688150832 took 32.51830339431763s\n",
      "29220: Done after 33 steps. Reward: 9.222266613706307\n",
      "29240: Done after 113 steps. Reward: 6.815473743084819\n",
      "29260: Done after 121 steps. Reward: 5.716017975637403\n",
      "29280: Done after 68 steps. Reward: 8.45705093667651\n",
      "29300: Done after 32 steps. Reward: 9.216485528524055\n",
      "29320: Done after 30 steps. Reward: 9.618778530054177\n",
      "29340: Done after 72 steps. Reward: -11.574552809408257\n",
      "29360: Done after 32 steps. Reward: 9.347361274169023\n",
      "29380: Done after 37 steps. Reward: 8.863155531270488\n",
      "29400: Done after 33 steps. Reward: 9.244232060610294\n",
      "Evaluation after 29400 episodes: -13.371259361635639 took 28.51847743988037s\n",
      "29420: Done after 58 steps. Reward: 9.555602583371861\n",
      "29440: Done after 251 steps. Reward: -6.803814522099458\n",
      "29460: Done after 32 steps. Reward: 9.558307101588646\n",
      "29480: Done after 33 steps. Reward: 9.215652452071055\n",
      "29500: Done after 36 steps. Reward: 8.903444195171447\n",
      "29520: Done after 64 steps. Reward: -12.559130490543732\n",
      "29540: Done after 27 steps. Reward: 9.721840222592796\n",
      "29560: Done after 31 steps. Reward: 9.536377175425127\n",
      "29580: Done after 33 steps. Reward: 9.22335781489333\n",
      "29600: Done after 28 steps. Reward: 9.700901036093658\n",
      "Evaluation after 29600 episodes: -13.693405631370323 took 34.315638303756714s\n",
      "29620: Done after 49 steps. Reward: -13.170925391200797\n",
      "29640: Done after 251 steps. Reward: -25.00529095579659\n",
      "29660: Done after 31 steps. Reward: 9.638997074713792\n",
      "29680: Done after 140 steps. Reward: 6.089564010624176\n",
      "29700: Done after 31 steps. Reward: 9.656244808295975\n",
      "29720: Done after 75 steps. Reward: -11.788525233752443\n",
      "29740: Done after 70 steps. Reward: 8.905186167048065\n",
      "29760: Done after 32 steps. Reward: 9.41286072573253\n",
      "29780: Done after 30 steps. Reward: 9.516891744458711\n",
      "29800: Done after 201 steps. Reward: -20.113574413099755\n",
      "Evaluation after 29800 episodes: -15.791481201738772 took 34.717957735061646s\n",
      "29820: Done after 31 steps. Reward: 9.316167074948265\n",
      "29840: Done after 251 steps. Reward: -0.8641124160421441\n",
      "29860: Done after 26 steps. Reward: 9.723670413008094\n",
      "29880: Done after 251 steps. Reward: -10.416661063243556\n",
      "29900: Done after 34 steps. Reward: 9.146388568491066\n",
      "29920: Done after 118 steps. Reward: -16.21464497109894\n",
      "29940: Done after 31 steps. Reward: 9.43189262546445\n",
      "29960: Done after 251 steps. Reward: -47.2471191286913\n",
      "29980: Done after 251 steps. Reward: -41.997907602998865\n",
      "30000: Done after 57 steps. Reward: 9.28794846575298\n",
      "Evaluation after 30000 episodes: -24.477611443628025 took 33.351582288742065s\n",
      "30020: Done after 30 steps. Reward: 9.388911691689723\n",
      "30040: Done after 66 steps. Reward: 8.939889251033152\n",
      "30060: Done after 34 steps. Reward: 9.088933893377437\n",
      "30080: Done after 184 steps. Reward: 2.7040163497095087\n",
      "30100: Done after 60 steps. Reward: 9.095666240257543\n",
      "30120: Done after 31 steps. Reward: 9.50997276207115\n",
      "30140: Done after 30 steps. Reward: 9.54721141189574\n",
      "30160: Done after 31 steps. Reward: 9.572998032060644\n",
      "30180: Done after 87 steps. Reward: -12.872715238000845\n",
      "30200: Done after 251 steps. Reward: -49.237876327062594\n",
      "Evaluation after 30200 episodes: -25.797026701731152 took 35.693050146102905s\n",
      "30220: Done after 31 steps. Reward: 9.527372725344357\n",
      "30240: Done after 35 steps. Reward: 9.239024621973066\n",
      "30260: Done after 131 steps. Reward: 9.558300452819685\n",
      "30280: Done after 31 steps. Reward: 9.467805050568247\n",
      "30300: Done after 30 steps. Reward: 9.541532966426287\n",
      "30320: Done after 29 steps. Reward: 9.633094532546737\n",
      "30340: Done after 58 steps. Reward: 9.43883917986909\n",
      "30360: Done after 32 steps. Reward: 9.427655505883111\n",
      "30380: Done after 68 steps. Reward: 8.720509410520433\n",
      "30400: Done after 33 steps. Reward: 9.387319415939288\n",
      "Evaluation after 30400 episodes: -15.466072924914842 took 30.236003875732422s\n",
      "30420: Done after 29 steps. Reward: 9.624259007050343\n",
      "30440: Done after 123 steps. Reward: -18.24826498634325\n",
      "30460: Done after 34 steps. Reward: 9.158364247561565\n",
      "30480: Done after 61 steps. Reward: 8.965377157819594\n",
      "30500: Done after 31 steps. Reward: 9.598388889414878\n",
      "30520: Done after 30 steps. Reward: 9.577232888674283\n",
      "30540: Done after 120 steps. Reward: -14.819289835192954\n",
      "30560: Done after 26 steps. Reward: 9.738866961344064\n",
      "30580: Done after 74 steps. Reward: 9.514560256201706\n",
      "30600: Done after 29 steps. Reward: 9.672284010132516\n",
      "Evaluation after 30600 episodes: -24.35785115690028 took 35.57143950462341s\n",
      "30620: Done after 52 steps. Reward: -12.843305857825063\n",
      "30640: Done after 32 steps. Reward: 9.412813914642067\n",
      "30660: Done after 31 steps. Reward: 9.505518164171013\n",
      "30680: Done after 30 steps. Reward: 9.546587260916434\n",
      "30700: Done after 61 steps. Reward: 9.314752636805979\n",
      "30720: Done after 79 steps. Reward: -12.309617684412903\n",
      "30740: Done after 114 steps. Reward: 6.213887811045833\n",
      "30760: Done after 29 steps. Reward: 9.539346327928758\n",
      "30780: Done after 33 steps. Reward: 9.297396864533091\n",
      "30800: Done after 181 steps. Reward: -22.04873619056346\n",
      "Evaluation after 30800 episodes: -15.612326493841108 took 34.001521825790405s\n",
      "30820: Done after 179 steps. Reward: 3.8353154000997103\n",
      "30840: Done after 251 steps. Reward: -19.484861032604137\n",
      "30860: Done after 33 steps. Reward: 9.327687308568365\n",
      "30880: Done after 29 steps. Reward: 9.60337245699176\n",
      "30900: Done after 32 steps. Reward: 9.398472138071691\n",
      "30920: Done after 139 steps. Reward: 5.453014527605926\n",
      "30940: Done after 59 steps. Reward: 9.274868184772423\n",
      "30960: Done after 33 steps. Reward: 9.255265534415338\n",
      "30980: Done after 140 steps. Reward: -16.54171274797992\n",
      "31000: Done after 251 steps. Reward: -8.346855319689597\n",
      "Evaluation after 31000 episodes: -18.560950804397525 took 32.04970145225525s\n",
      "31020: Done after 32 steps. Reward: 9.26820748791441\n",
      "31040: Done after 47 steps. Reward: -11.888052392461521\n",
      "31060: Done after 30 steps. Reward: 9.514957389046527\n",
      "31080: Done after 37 steps. Reward: 9.009498038125617\n",
      "31100: Done after 28 steps. Reward: 9.608230800731354\n",
      "31120: Done after 34 steps. Reward: 9.249446892674165\n",
      "31140: Done after 69 steps. Reward: 9.265176912097282\n",
      "31160: Done after 34 steps. Reward: 9.013646833455606\n",
      "31180: Done after 61 steps. Reward: 9.271481462255927\n",
      "31200: Done after 31 steps. Reward: 9.467452357753519\n",
      "Evaluation after 31200 episodes: -16.108766910021178 took 30.425158500671387s\n",
      "31220: Done after 45 steps. Reward: -11.674374609729632\n",
      "31240: Done after 35 steps. Reward: 9.189332054096406\n",
      "31260: Done after 65 steps. Reward: 8.446652348739713\n",
      "31280: Done after 33 steps. Reward: 9.23352332343534\n",
      "31300: Done after 251 steps. Reward: -5.784230120914046\n",
      "31320: Done after 30 steps. Reward: 9.513014422204224\n",
      "31340: Done after 31 steps. Reward: 9.343919959767709\n",
      "31360: Done after 129 steps. Reward: 8.653201614535261\n",
      "31380: Done after 251 steps. Reward: -1.9576897473262198\n",
      "31400: Done after 251 steps. Reward: -37.171791865435154\n",
      "Evaluation after 31400 episodes: -14.809121504454902 took 33.341315507888794s\n",
      "31420: Done after 33 steps. Reward: 9.437537281612252\n",
      "31440: Done after 31 steps. Reward: 9.505171650465686\n",
      "31460: Done after 30 steps. Reward: 9.547524727769426\n",
      "31480: Done after 26 steps. Reward: 9.733124797845937\n",
      "31500: Done after 29 steps. Reward: 9.614347464656714\n",
      "31520: Done after 51 steps. Reward: -12.239186345786619\n",
      "31540: Done after 36 steps. Reward: 8.818291645957856\n",
      "31560: Done after 89 steps. Reward: 9.127885301514478\n",
      "31580: Done after 32 steps. Reward: 9.294055819306356\n",
      "31600: Done after 67 steps. Reward: -15.289144877949074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m state, _info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 87\u001b[0m     a \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     88\u001b[0m     a \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mdiscrete_to_continous_action(a)\n\u001b[1;32m     89\u001b[0m     a_agent2 \u001b[38;5;241m=\u001b[39m opponent_weak\u001b[38;5;241m.\u001b[39mact(env\u001b[38;5;241m.\u001b[39mobs_agent_two())\n",
      "Cell \u001b[0;32mIn[35], line 50\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\u001b[38;5;241m.\u001b[39mgreedy_action(state)\n",
      "Cell \u001b[0;32mIn[30], line 48\u001b[0m, in \u001b[0;36mQFunction.greedy_action\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreedy_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, states):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(states), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 84\u001b[0m, in \u001b[0;36mFeedforward.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 84\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "losses = []\n",
    "\n",
    "last_eval = float(\"-inf\")\n",
    "\n",
    "for i in range(config[\"max_episodes\"]):\n",
    "    # First explore the environment for one whole episode\n",
    "    total_reward = 0\n",
    "    self_play = False\n",
    "\n",
    "    state, _info = env.reset()\n",
    "    state_agent2 = env.obs_agent_two()\n",
    "\n",
    "    if np.random.rand() < config[\"weak_percent\"]:\n",
    "        opponent = opponent_weak\n",
    "    elif np.random.rand() < config[\"self_percent\"]:\n",
    "        opponent = agent\n",
    "        self_play = True\n",
    "    else:\n",
    "        opponent = opponent_strong\n",
    "\n",
    "    for t in range(config[\"max_steps\"]):\n",
    "        done = False\n",
    "        # Agent chooses action with epsilon-greedy policy\n",
    "        a_discrete = agent.act(state)\n",
    "        a = discrete_to_continous_action(a_discrete)\n",
    "\n",
    "        # if np.random.rand() < opponent_percent:\n",
    "        #     a_agent2 = opponent_weak.act(state_agent2)\n",
    "        # elif np.random.rand() < self_percent:\n",
    "        #     a_agent2 = env.discrete_to_continous_action(agent.act(state_agent2))\n",
    "        # else:\n",
    "        #     a_agent2 = opponent_strong.act(state)\n",
    "\n",
    "        if self_play:\n",
    "            a_agent2 = discrete_to_continous_action(agent.act(state_agent2))\n",
    "        else:\n",
    "            a_agent2 = opponent.act(state_agent2)\n",
    "\n",
    "        (state_new, reward, done, trunc, _info) = env.step(np.hstack([a, a_agent2]))\n",
    "        state_agent2 = env.obs_agent_two()\n",
    "\n",
    "        total_reward += reward\n",
    "        agent.store_transition((state, a_discrete, reward, state_new, done))\n",
    "        state = state_new\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Train agent for (iter_fit) iterations\n",
    "    episode_losses = agent.train()\n",
    "    losses.extend(episode_losses)\n",
    "    stats.append(total_reward)\n",
    "\n",
    "    # Write to tensorboard\n",
    "    writer.add_scalar(\"training/loss\", np.mean(episode_losses), i)\n",
    "    writer.add_scalar(\"training/reward\", total_reward, i)\n",
    "    writer.add_scalar(\"training/epsilon\", agent.eps, i)\n",
    "    writer.add_scalar(\"training/steps\", t+1, i)\n",
    "\n",
    "    # Print if necessray\n",
    "    if i % config[\"print_every\"] == 0:\n",
    "        print(\"{}: Done after {} steps. Reward: {}\".format(i, t+1, total_reward))\n",
    "\n",
    "    # Evaluate agent\n",
    "    if i % config[\"eval_every\"] == 0:\n",
    "        start_ts = time.time()\n",
    "        total_reward = 0\n",
    "        wins = 0\n",
    "        for _ in range(config[\"eval_episodes\"]):\n",
    "            state, _info = env.reset()\n",
    "            for t in range(config[\"max_steps\"]):\n",
    "                a = agent.act(state, eps=0)\n",
    "                a = env.discrete_to_continous_action(a)\n",
    "                a_agent2 = opponent_strong.act(env.obs_agent_two())\n",
    "                (state, reward, done, trunc, _info) = env.step(np.hstack([a, a_agent2]))\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            if _info[\"winner\"] == 1:\n",
    "                wins += 1\n",
    "        total_reward /= config[\"eval_episodes\"]\n",
    "\n",
    "        wins_weak = 0\n",
    "        for _ in range(config[\"eval_episodes\"]):\n",
    "            state, _info = env.reset()\n",
    "            for t in range(config[\"max_steps\"]):\n",
    "                a = agent.act(state, eps=0)\n",
    "                a = env.discrete_to_continous_action(a)\n",
    "                a_agent2 = opponent_weak.act(env.obs_agent_two())\n",
    "                (state, reward, done, trunc, _info) = env.step(np.hstack([a, a_agent2]))\n",
    "                if done:\n",
    "                    break\n",
    "            if _info[\"winner\"] == 1:\n",
    "                wins_weak += 1\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"training/eval\", total_reward, i)\n",
    "        writer.add_scalar(\"training/eval_wins\", (wins + wins_weak), i)\n",
    "        writer.add_scalar(\"training/eval_winrate\", wins/(float(config[\"eval_episodes\"])), i)\n",
    "        writer.add_scalar(\"training/eval_winrate_weak\", wins_weak/(float(config[\"eval_episodes\"])), i)\n",
    "        print(\"Evaluation after {} episodes: {} took {}s\".format(i, total_reward, (time.time()-start_ts)))\n",
    "\n",
    "        total_reward = (wins_weak + wins*2.0) / (3.0 * config[\"eval_episodes\"])\n",
    "        if total_reward > last_eval:\n",
    "            percent = (total_reward - last_eval) / last_eval * 100\n",
    "            print(f\"New best model with {percent:.2f}% improvement... Saving model as checkpoint\")\n",
    "            _cp_path = checkpoint_path + str(checkpoint_idx) + \".pt\"\n",
    "            torch.save(agent.Q.state_dict(), _cp_path)\n",
    "            print(f\"Saved model to {_cp_path}\")\n",
    "            checkpoint_idx = (checkpoint_idx + 1) % checkpoint_count\n",
    "            last_eval = total_reward\n",
    "\n",
    "    # Save model every 1000 episodes\n",
    "    if i % 1000 == 0:\n",
    "        torch.save(agent.Q.state_dict(), checkpoint_path + f\"{i}_eps\" + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
